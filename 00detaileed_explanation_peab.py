# -*- coding: utf-8 -*-
"""
Script para anÃ¡lise detalhada de instÃ¢ncias especÃ­ficas usando cache cumulativo.
"""

import os
import sys
from pathlib import Path
import joblib
import json
import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from datetime import datetime

# Constante para o cache cumulativo
CACHE_FILE = Path("auxiliary_files/cache_cumulativo.pkl")

def load_cache() -> dict:
    """Carrega o cache cumulativo."""
    try:
        if not CACHE_FILE.exists():
            print(f"âŒ Erro: Cache nÃ£o encontrado em {CACHE_FILE}")
            print("Execute primeiro o script peab_comparation.py para gerar o cache.")
            sys.exit(1)
        
        cache = joblib.load(CACHE_FILE)
        if not isinstance(cache, dict) or not cache:
            raise ValueError("Cache invÃ¡lido ou vazio")
            
        return cache
        
    except Exception as e:
        print(f"âŒ Erro ao carregar cache: {e}")
        print("Execute novamente o script peab_comparation.py para gerar o cache.")
        sys.exit(1)

def list_available_datasets(cache: dict) -> None:
    """Lista os datasets disponÃ­veis no cache."""
    print("\nğŸ“Š Datasets disponÃ­veis no cache:")
    for i, dataset in enumerate(sorted(cache.keys()), 1):
        stats = cache[dataset].get('stats', {})
        total = len(cache[dataset]['X_test'])
        print(f"{i}. {dataset:<25} ({total} instÃ¢ncias)")
    print()

def format_line():
    return "=" * 72

def salvar_relatorios(dataset_nome, idx, explicacao_tecnica, explicacao_academica, dados_json):
    """Salva relatÃ³rios tÃ©cnico, acadÃªmico e JSON para uma instÃ¢ncia."""
    pasta = os.path.join("explicacoes_detalhadas", dataset_nome, f"instancia_{idx}")
    os.makedirs(pasta, exist_ok=True)
    
    with open(os.path.join(pasta, "explicacao_tecnica.txt"), "w", encoding="utf-8") as f:
        f.write(explicacao_tecnica)
    with open(os.path.join(pasta, "explicacao_academica.txt"), "w", encoding="utf-8") as f:
        f.write(explicacao_academica)
    with open(os.path.join(pasta, "explicacao_dados.json"), "w", encoding="utf-8") as f:
        json.dump(dados_json, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… RelatÃ³rios salvos em: {pasta}\n")

def processar_instancia(dataset_nome, cache_dataset, pipeline, X_test, y_test, t_plus, t_minus, nomes_classes, feature_names, idx):
    """Processa uma instÃ¢ncia especÃ­fica e gera explicaÃ§Ãµes."""
    try:
        # Prepara a instÃ¢ncia para anÃ¡lise
        instance = X_test.iloc[[idx]]
        true_class = y_test.iloc[idx]
        
        # ObtÃ©m a prediÃ§Ã£o e o score
        pred_class = pipeline.predict(instance)[0]
        score = pipeline.decision_function(instance)[0]
        
        # Determina o status da instÃ¢ncia (aceita/rejeitada)
        rejected = (score > t_minus and score < t_plus)
        status = "REJEITADA" if rejected else "ACEITA"
        
        # Extrai componentes do pipeline e verifica se sÃ£o vÃ¡lidos
        try:
            scaler = pipeline.named_steps['scaler']
            classifier = pipeline.named_steps['modelo']
        except KeyError as e:
            raise ValueError(f"Erro ao acessar componentes do pipeline: {e}")
        
        # Coleta dados para explicaÃ§Ã£o com verificaÃ§Ãµes
        scaled_instance = scaler.transform(instance)
        coefs = classifier.coef_[0]
        
        # Calcula contribuiÃ§Ãµes
        contributions = coefs * scaled_instance[0]
        sorted_idx = np.argsort(np.abs(contributions))[::-1]
        
        # Gera explicaÃ§Ã£o tÃ©cnica
        explicacao_tecnica = f"""
=== ANÃLISE TÃ‰CNICA DA INSTÃ‚NCIA {idx} ===
Dataset: {dataset_nome}
Status: {status}
Classe Real: {nomes_classes[true_class]}
{'Classe Predita: ' + str(nomes_classes[pred_class]) if not rejected else 'PrediÃ§Ã£o: REJEITADA'}
Score: {score:.4f}
Thresholds: t+ = {t_plus:.4f}, t- = {t_minus:.4f}

Top ContribuiÃ§Ãµes:
"""
        for i in sorted_idx[:5]:
            explicacao_tecnica += f"{feature_names[i]}: {contributions[i]:.4f}\n"
        
        # Gera explicaÃ§Ã£o acadÃªmica
        explicacao_academica = f"""
AnÃ¡lise MatemÃ¡tica Detalhada - InstÃ¢ncia {idx}
================================================
1. InformaÃ§Ãµes BÃ¡sicas:
   - Dataset: {dataset_nome}
   - Classe Verdadeira: {nomes_classes[true_class]}
   - Score de DecisÃ£o: {score:.4f}
   
2. AnÃ¡lise de RejeiÃ§Ã£o:
   - Threshold Superior (t+): {t_plus:.4f}
   - Threshold Inferior (t-): {t_minus:.4f}
   - DecisÃ£o: {status}
   
3. DecomposiÃ§Ã£o das ContribuiÃ§Ãµes:
"""
        for i in sorted_idx[:5]:
            explicacao_academica += f"   {feature_names[i]}: {contributions[i]:.4f}\n"
        
        # Prepara dados para JSON
        dados_json = {
            "dataset": dataset_nome,
            "instancia": int(idx),
            "classe_real": str(nomes_classes[true_class]),
            "classe_predita": str(nomes_classes[pred_class]) if not rejected else "REJEITADA",
            "score": float(score),
            "thresholds": {
                "t_plus": float(t_plus),
                "t_minus": float(t_minus)
            },
            "status": status,
            "contribuicoes": {
                str(feature_names[i]): float(contributions[i])
                for i in sorted_idx[:5]
            }
        }
        
        # Salva os relatÃ³rios
        salvar_relatorios(dataset_nome, idx, explicacao_tecnica, explicacao_academica, dados_json)
        
    except Exception as e:
        print(f"âŒ Erro ao processar instÃ¢ncia {idx}: {e}")
        return False
    
    return True

def run_prova_detalhada():
    """FunÃ§Ã£o principal para anÃ¡lise detalhada de instÃ¢ncias."""
    print(format_line())
    print("ğŸ§© PROVA MATEMÃTICA DETALHADA DE UMA INSTÃ‚NCIA")
    
    try:
        # Carrega o cache cumulativo
        cache_completo = load_cache()
        
        # Lista datasets disponÃ­veis
        list_available_datasets(cache_completo)
        
        # SeleÃ§Ã£o do dataset
        while True:
            try:
                escolha = input("Digite o nÃºmero do dataset desejado: ")
                datasets = sorted(cache_completo.keys())
                dataset_nome = datasets[int(escolha) - 1]
                break
            except (ValueError, IndexError):
                print("âŒ Escolha invÃ¡lida. Tente novamente.")
        
        try:
            # ObtÃ©m dados do cache para o dataset selecionado
            cache_dataset = cache_completo[dataset_nome]
            
            # Verifica componentes necessÃ¡rios
            required_keys = ['pipeline_modelo', 'X_test', 'y_test', 't_plus', 't_minus', 'nomes_classes', 'feature_names']
            missing_keys = [k for k in required_keys if k not in cache_dataset]
            if missing_keys:
                raise KeyError(f"Dados ausentes no cache: {', '.join(missing_keys)}")
            
            # Carrega o pipeline
            pipeline = cache_dataset['pipeline_modelo']
            
            # ReconstrÃ³i os DataFrames e Series
            feature_names = cache_dataset['feature_names']
            
            # ReconstrÃ³i X_test e y_test
            try:
                X_test = pd.DataFrame.from_dict(cache_dataset['X_test'])
                X_test.columns = feature_names
                y_test = pd.Series(cache_dataset['y_test'])
            except Exception as e:
                raise ValueError(f"Erro ao reconstruir dados de teste: {e}")
            
            # ReconstrÃ³i X_train e y_train
            try:
                X_train = pd.DataFrame.from_dict(cache_dataset['X_train'])
                X_train.columns = feature_names
                y_train = pd.Series(cache_dataset['y_train'])
            except Exception as e:
                raise ValueError(f"Erro ao reconstruir dados de treino: {e}")
            
            # Carrega outros parÃ¢metros
            t_plus = float(cache_dataset['t_plus'])
            t_minus = float(cache_dataset['t_minus'])
            nomes_classes = cache_dataset['nomes_classes']
            
            # Mostra informaÃ§Ãµes sobre o dataset
            print(f"\nğŸ“Š Dataset: {dataset_nome}")
            print(f"Total de instÃ¢ncias de teste: {len(X_test)}")
            print(f"Classes disponÃ­veis: {', '.join([str(nc) for nc in nomes_classes])}")
            
            # SeleÃ§Ã£o da instÃ¢ncia
            while True:
                try:
                    idx = int(input("\nDigite o nÃºmero da instÃ¢ncia para analisar (0 atÃ© {}): ".format(len(X_test)-1)))
                    if 0 <= idx < len(X_test):
                        break
                    print("âŒ Ãndice fora do intervalo vÃ¡lido.")
                except ValueError:
                    print("âŒ Por favor, digite um nÃºmero vÃ¡lido.")
            
            # Processa a instÃ¢ncia selecionada
            processar_instancia(dataset_nome, cache_dataset, pipeline, X_test, y_test,
                              t_plus, t_minus, nomes_classes, feature_names, idx)
            
        except Exception as e:
            print(f"âŒ Erro ao processar dataset {dataset_nome}: {e}")
            return
        
    except Exception as e:
        print(f"âŒ Erro geral: {e}")
        return

if __name__ == "__main__":
    try:
        run_prova_detalhada()
    except KeyboardInterrupt:
        print("\n\nâŒ OperaÃ§Ã£o cancelada pelo usuÃ¡rio.")
    except Exception as e:
        print(f"\n\nâŒ Erro: {str(e)}")
    finally:
        print("\n" + format_line())