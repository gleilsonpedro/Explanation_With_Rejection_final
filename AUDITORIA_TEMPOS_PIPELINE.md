# ðŸ” AUDITORIA DE TEMPOS E PIPELINE - AnÃ¡lise Comparativa

**Data**: 09/12/2025  
**Objetivo**: Garantir mediÃ§Ã£o precisa de tempo e consistÃªncia de pipeline

---

## ðŸ“Š RESUMO DA AUDITORIA

| MÃ©todo | Tempo Medido | Pipeline | Status |
|--------|-------------|----------|--------|
| **PEAB** | âœ… Apenas experimento | âœ… PrÃ³prio | âœ… CORRETO |
| **PuLP** | âœ… Apenas experimento | âœ… Usa PEAB | âœ… CORRETO |
| **Anchor** | âš ï¸ Inclui explainer.explain() | âœ… Usa shared_pipeline | âš ï¸ CORRIGIR |
| **MinExp** | âš ï¸ Inclui chunks inteiros | âœ… Usa shared_pipeline | âš ï¸ CORRIGIR |

---

## ðŸ”¬ ANÃLISE DETALHADA POR MÃ‰TODO

### 1ï¸âƒ£ PEAB (peab.py) âœ…

**MediÃ§Ã£o de Tempo**:
```python
# Linha 840-872
start_total = time.perf_counter()
with ProgressBar(total=len(X_test)) as pbar:
    for i in range(len(X_test)):
        start_inst = time.perf_counter()  # âœ… INÃCIO CORRETO
        inst = X_test.iloc[[i]]
        expl, logs, ad, rm = gerar_explicacao_instancia(...)  # APENAS ISTO
        duracao = time.perf_counter() - start_inst  # âœ… FIM CORRETO
        # ... resto do cÃ³digo (nÃ£o inclui na mediÃ§Ã£o)
total_time_experimento = time.perf_counter() - start_total
```

**âœ… CORRETO**: 
- Timer inicia DEPOIS de preparar `inst`
- Timer para IMEDIATAMENTE apÃ³s `gerar_explicacao_instancia()`
- Barra de progresso (`pbar.update()`) NÃƒO incluÃ­da
- Append de resultados NÃƒO incluÃ­do

**Pipeline**: PrÃ³prio (MinMaxScaler + LogisticRegression)

---

### 2ï¸âƒ£ PuLP (pulp_experiment.py) âœ…

**MediÃ§Ã£o de Tempo**:
```python
# Linha 253-257
start_time = time.perf_counter()  # âœ… INÃCIO CORRETO
features_otimas, tamanho, tipo_pred = calcular_explicacao_otima_pulp(
    modelo, instancia, X_train, t_plus, t_minus
)
tempo_gasto = time.perf_counter() - start_time  # âœ… FIM CORRETO
```

**âœ… CORRETO**:
- Mede APENAS `calcular_explicacao_otima_pulp()`
- Barra de progresso (`pbar.update()`) NÃƒO incluÃ­da
- AtualizaÃ§Ã£o de estatÃ­sticas NÃƒO incluÃ­da

**Pipeline**: Usa `treinar_e_avaliar_modelo()` do PEAB âœ…

---

### 3ï¸âƒ£ Anchor (anchor.py) âš ï¸

**MediÃ§Ã£o de Tempo**:
```python
# Linha 217-253
start_time = time.time()  # âš ï¸ ANTES DA PREPARAÃ‡ÃƒO
instance_arr = X_test.iloc[i].values if hasattr(X_test, 'iloc') else X_test[i]

try:
    explanation = explainer.explain(...)  # ALGORITMO
except ...
    # MUITOS TRY/EXCEPT
    
runtime = time.time() - start_time  # âš ï¸ INCLUI PREPARAÃ‡ÃƒO + EXCEÃ‡Ã•ES
```

**âš ï¸ PROBLEMAS**:
1. Timer inicia ANTES de `instance_arr = X_test.iloc[i].values`
2. Inclui tempo de conversÃ£o de dados
3. Inclui tempo dos `try/except` (mesmo se falhar)
4. Usa `time.time()` em vez de `time.perf_counter()` (menos preciso)

**Pipeline**: Usa `get_shared_pipeline()` âœ… (consistente com PEAB)

**ðŸ”§ CORREÃ‡ÃƒO NECESSÃRIA**:
```python
instance_arr = X_test.iloc[i].values if hasattr(X_test, 'iloc') else X_test[i]
start_time = time.perf_counter()  # MOVER PARA CÃ
try:
    explanation = explainer.explain(...)
    runtime = time.perf_counter() - start_time  # DENTRO DO TRY
except:
    runtime = 0.0  # ou np.nan
```

---

### 4ï¸âƒ£ MinExp (minexp.py) âš ï¸

**MediÃ§Ã£o de Tempo**:
```python
# Linha 187-197
start_time_neg = time.time()  # âš ï¸ ANTES DO CHUNKING
if len(neg_idx) > 0:
    explain_in_chunks(neg_idx, "Negative")  # CHUNKS + PROGRESS BAR
runtime_neg = time.time() - start_time_neg  # âš ï¸ INCLUI OVERHEAD

# Linha 201-234
start_time_rej = time.time()  # âš ï¸ ANTES DO LOOP
if len(rej_idx) > 0:
    for start in range(0, len(rej_idx), chunk_size):  # LOOP + TRY/EXCEPT
        sl = slice(start, start + chunk_size)
        sel_idx = rej_idx[sl]
        try:
            explanations_local = utils.svm_explainer.svm_explanation_rejected(...)
            # ... processamento
runtime_rej = time.time() - start_time_rej  # âš ï¸ INCLUI CHUNKING + ERROS
```

**âš ï¸ PROBLEMAS**:
1. Mede tempo de CHUNKS inteiros (nÃ£o por instÃ¢ncia individual)
2. Inclui overhead de chunking (loops, slicing)
3. Inclui tempo de `try/except` mesmo quando falha
4. Inclui tempo de `pbar.update()` dentro dos chunks
5. Usa `time.time()` em vez de `time.perf_counter()`

**Pipeline**: Usa `get_shared_pipeline()` âœ… (consistente com PEAB)

**ðŸ”§ CORREÃ‡ÃƒO NECESSÃRIA**:
```python
# Medir por instÃ¢ncia individualmente
tempos_por_instancia = {}
for idx in neg_idx:
    start_time = time.perf_counter()
    try:
        explanation = utils.svm_explainer.svm_explanation_negative(...)
        tempo = time.perf_counter() - start_time
    except:
        tempo = np.nan
    tempos_por_instancia[idx] = tempo
```

---

## ðŸŽ¯ IMPACTO DA INCONSISTÃŠNCIA

### ComparaÃ§Ã£o Atual (ANTES DA CORREÃ‡ÃƒO):

| MÃ©trica | PEAB | PuLP | Anchor | MinExp |
|---------|------|------|--------|--------|
| **O que mede** | Apenas algoritmo | Apenas algoritmo | Algoritmo + overhead | Chunks + overhead |
| **PrecisÃ£o** | Alta | Alta | MÃ©dia | Baixa |
| **Comparabilidade** | âœ… Baseline | âœ… Justo vs PEAB | âš ï¸ Inflado | âš ï¸ Muito inflado |

**Exemplo HipotÃ©tico**:
```
PEAB:   0.100s (puro)
PuLP:   0.500s (puro)
Anchor: 0.250s (0.200s puro + 0.050s overhead)  â† INJUSTO
MinExp: 1.500s (1.000s puro + 0.500s chunking) â† MUITO INJUSTO
```

**ConclusÃ£o Errada**: "Anchor Ã© 2.5x mais lento que PEAB"  
**Realidade**: "Anchor Ã© 2.0x mais lento que PEAB"

---

## ðŸ“‹ PLANO DE CORREÃ‡ÃƒO

### âœ… PRIORIDADE ALTA

#### 1. Anchor - Mover timer para depois da preparaÃ§Ã£o
```python
# ANTES (ERRADO)
start_time = time.time()
instance_arr = X_test.iloc[i].values

# DEPOIS (CORRETO)
instance_arr = X_test.iloc[i].values
start_time = time.perf_counter()
```

#### 2. Anchor - Usar time.perf_counter() em vez de time.time()
```python
# ANTES
start_time = time.time()
runtime = time.time() - start_time

# DEPOIS
start_time = time.perf_counter()
runtime = time.perf_counter() - start_time
```

#### 3. MinExp - Medir por instÃ¢ncia individualmente
```python
# CRIAR DICIONÃRIO DE TEMPOS
tempos_individuais = {}

# DENTRO DO LOOP DE EXPLICAÃ‡ÃƒO
for idx in indices:
    start = time.perf_counter()
    try:
        exp = explicar(idx)
        tempo = time.perf_counter() - start
    except:
        tempo = np.nan
    tempos_individuais[idx] = tempo
```

### âš™ï¸ PRIORIDADE MÃ‰DIA

#### 4. Padronizar time.perf_counter() em todos
- âœ… PEAB: jÃ¡ usa
- âœ… PuLP: jÃ¡ usa
- âš ï¸ Anchor: mudar de `time.time()` â†’ `time.perf_counter()`
- âš ï¸ MinExp: mudar de `time.time()` â†’ `time.perf_counter()`

**Justificativa**: `time.perf_counter()` tem maior resoluÃ§Ã£o e nÃ£o Ã© afetado por ajustes de relÃ³gio do sistema.

---

## âœ… VALIDAÃ‡ÃƒO DE PIPELINE (CONSISTÃŠNCIA)

### Pipeline de Treino:

| Componente | PEAB | PuLP | Anchor | MinExp |
|-----------|------|------|--------|--------|
| **Scaler** | MinMaxScaler | MinMaxScaler | MinMaxScaler | MinMaxScaler |
| **Modelo** | LogisticRegression | LogisticRegression | LogisticRegression | LogisticRegression |
| **Origem** | PrÃ³prio | Usa PEAB | shared_training | shared_training |
| **HiperparÃ¢metros** | hiperparametros.json | hiperparametros.json | hiperparametros.json | hiperparametros.json |
| **Random State** | 42 | 42 | 42 | 42 |
| **Thresholds** | Grid adaptativo | Usa PEAB | Usa PEAB | Usa PEAB |

**âœ… TODOS CONSISTENTES**: Anchor e MinExp usam `get_shared_pipeline()` que chama `treinar_e_avaliar_modelo()` do PEAB.

### Top-K Features:

âœ… **CORRETO**: `shared_training.py` aplica `top_k_features` ANTES do treino:
```python
# Linha 45-87
top_k = cfg.get('top_k_features', None)
if top_k and top_k > 0 and top_k < X.shape[1]:
    from peab import aplicar_selecao_top_k_features
    X_train, X_test, selected_features = aplicar_selecao_top_k_features(...)
```

---

## ðŸŽ“ RECOMENDAÃ‡Ã•ES PARA PAPER

### O que REPORTAR:

âœ… **Correto**:
- "Todos os mÃ©todos usam o mesmo pipeline de treino (MinMaxScaler + LogisticRegression)"
- "HiperparÃ¢metros idÃªnticos carregados de hiperparametros.json"
- "Thresholds t+/t- calculados uma Ãºnica vez (PEAB) e reutilizados"
- "Split determinÃ­stico com RANDOM_STATE=42"

âš ï¸ **APÃ“S CORREÃ‡ÃƒO**:
- "Tempos medidos com time.perf_counter() (resoluÃ§Ã£o de nanosegundos)"
- "MediÃ§Ã£o exclui overhead de I/O, logging e progress bars"
- "Cada instÃ¢ncia medida individualmente para comparaÃ§Ã£o justa"

âŒ **NÃƒO REPORTAR (antes da correÃ§Ã£o)**:
- ~~"Anchor Ã© Xx mais lento"~~ (tempos inflados)
- ~~"MinExp processa em chunks de Y instÃ¢ncias"~~ (irrelevante para comparaÃ§Ã£o)

---

## ðŸ“Š CHECKLIST DE VALIDAÃ‡ÃƒO

### Antes de Rodar Experimentos Finais:

- [x] **PEAB**: Timer correto âœ…
- [x] **PuLP**: Timer correto âœ…
- [ ] **Anchor**: Mover timer + usar perf_counter
- [ ] **MinExp**: Medir por instÃ¢ncia + usar perf_counter
- [x] **Pipeline**: Todos usam shared_training âœ…
- [x] **HiperparÃ¢metros**: Todos usam hiperparametros.json âœ…
- [x] **Random State**: Todos usam 42 âœ…
- [x] **Thresholds**: Anchor/MinExp reutilizam PEAB âœ…

### Depois das CorreÃ§Ãµes:

- [ ] Re-executar Anchor em dataset de teste
- [ ] Re-executar MinExp em dataset de teste
- [ ] Comparar tempos antes/depois da correÃ§Ã£o
- [ ] Validar que tempos sÃ£o comparÃ¡veis com PEAB
- [ ] Atualizar JSONs com tempos corrigidos

---

## ðŸ“ CONCLUSÃƒO

**Status Atual**:
- âœ… Pipeline: 100% consistente entre mÃ©todos
- âš ï¸ MediÃ§Ã£o de tempo: Inconsistente entre mÃ©todos

**Impacto**:
- **Alto** para comparaÃ§Ãµes de tempo (speedup, tempo mÃ©dio)
- **Baixo** para comparaÃ§Ãµes de qualidade (tamanho de explicaÃ§Ãµes, GAP)

**AÃ§Ã£o Requerida**:
1. Corrigir Anchor (2 mudanÃ§as simples)
2. Corrigir MinExp (refatoraÃ§Ã£o maior)
3. Re-executar experimentos
4. Atualizar JSONs

**Prioridade**: ALTA (antes de submeter paper/dissertaÃ§Ã£o)

---

**Autor**: Claude (GitHub Copilot)  
**Data**: 09/12/2025  
**VersÃ£o**: 1.0
