# üö® PROBLEMA CR√çTICO ENCONTRADO: Inst√¢ncias Rejeitadas Incorretas

**Data:** 12 de dezembro de 2025  
**Status:** ‚ö†Ô∏è REQUER CORRE√á√ÉO URGENTE

---

## üìã Resumo Executivo

**54% (24/44) das inst√¢ncias marcadas como "rejeitadas" NO est√£o na zona de rejei√ß√£o!**

Isso explica completamente por que a fidelidade de predi√ß√µes rejeitadas √© apenas ~23%.

---

## üîç O Que Foi Descoberto

### Teste Realizado
Verificamos se todas as 44 inst√¢ncias marcadas como `"rejected": true` no JSON est√£o realmente dentro da zona de rejei√ß√£o definida como `[-0.1096, 0.0779]`.

### Resultado
```
‚ùå 24/44 inst√¢ncias N√ÉO est√£o na zona de rejei√ß√£o
‚úì 20/44 inst√¢ncias est√£o corretamente na zona
```

### Exemplos de Inst√¢ncias Incorretas

| ID  | Score Salvo | Score Norm | Na Zona? | Deveria Ser       |
|-----|-------------|------------|----------|-------------------|
| 417 | 0.4390      | 0.1467     | ‚ùå N√ÉO   | **Positiva** (1)  |
| 78  | 0.4038      | 0.1429     | ‚ùå N√ÉO   | **Positiva** (1)  |
| 558 | 0.4501      | 0.1478     | ‚ùå N√ÉO   | **Positiva** (1)  |
| 351 | -0.6082     | (muito baixo) | ‚ùå N√ÉO   | **Negativa** (0)  |

**Zona de rejei√ß√£o:** [-0.1096, 0.0779]  
**Inst√¢ncias com score_norm > 0.0779:** Deveriam ser classificadas como POSITIVAS  
**Inst√¢ncias com score_norm < -0.1096:** Deveriam ser classificadas como NEGATIVAS

---

## üéØ Por Que Isso Causa Baixa Fidelidade

### O Teste de Fidelidade
A valida√ß√£o verifica se, ao perturbar features N√ÉO explicadas, a inst√¢ncia **continua na zona de rejei√ß√£o**.

### O Problema
Se uma inst√¢ncia tem score normalizado de **0.1467** (fora da zona que vai at√© 0.0779):
- Ela N√ÉO est√° rejeitada de fato
- Foi **marcada incorretamente** como rejeitada
- Quando perturbada, naturalmente sai da zona (porque j√° estava fora!)
- Resultado: **fidelidade baixa**

### Exemplo Concreto
```
Inst√¢ncia 417:
‚îú‚îÄ Score normalizado: 0.1467
‚îú‚îÄ Zona de rejei√ß√£o: [-0.1096, 0.0779]
‚îú‚îÄ Est√° na zona? N√ÉO! (0.1467 > 0.0779)
‚îú‚îÄ Mas est√° marcada como: rejected=true
‚îÇ
‚îú‚îÄ Ao validar fidelidade:
‚îÇ   ‚îú‚îÄ Perturba features n√£o explicadas
‚îÇ   ‚îú‚îÄ Espera que fique na zona [-0.1096, 0.0779]
‚îÇ   ‚îú‚îÄ Mas naturalmente fica em ~0.14-0.15 (onde sempre esteve!)
‚îÇ   ‚îî‚îÄ Resultado: FALHA (0% de fidelidade)
‚îÇ
‚îî‚îÄ CONCLUS√ÉO: Inst√¢ncia est√° INCORRETAMENTE marcada como rejeitada
```

---

## üêõ Onde Est√° o Bug?

O problema est√° na **inconsist√™ncia entre**:

1. **Thresholds usados no TREINO** (para encontrar t+ e t-)
2. **Thresholds usados no TESTE** (para classificar inst√¢ncias)

### C√≥digo Suspeito

Provavelmente em [utils/rejection_logic.py](utils/rejection_logic.py) ou no pr√≥prio [peab.py](peab.py), h√° uma diferen√ßa entre:

```python
# Durante o TREINO (correto)
t_plus_norm, t_minus_norm = encontrar_thresholds(...)  # Retorna em espa√ßo normalizado

# Durante o TESTE (incorreto?)
scores = model.decision_function(X_test)  # Scores SEM normaliza√ß√£o?
rejeitadas = (scores >= t_minus) & (scores <= t_plus)  # Compara ERRADO!
```

### Hip√≥tese
Os thresholds est√£o sendo salvos em **espa√ßo normalizado** mas aplicados a scores **n√£o normalizados** (ou vice-versa).

---

## ‚úÖ Como Corrigir

### 1. Localizar o Problema

Procurar em `peab.py` onde as inst√¢ncias s√£o classificadas como rejeitadas:

```python
# Linha ~351 em peab.py
is_rejected = t_minus <= score_norm <= t_plus  # ‚Üê Verificar se score_norm est√° correto
```

### 2. Garantir Consist√™ncia

Durante classifica√ß√£o, DEVE:
1. Calcular score do modelo: `score = model.decision_function(X)`
2. **Normalizar** usando mesmos par√¢metros do treino
3. Comparar com thresholds normalizados

```python
# CORRETO
score_raw = model.decision_function(X)
score_z = (score_raw - mean_score) / std_score
score_norm = score_z / max_abs
is_rejected = (t_minus <= score_norm <= t_plus)

# INCORRETO
score_raw = model.decision_function(X)
is_rejected = (t_minus <= score_raw <= t_plus)  # ‚Üê Compara raw com norm!
```

### 3. Re-executar PEAB

Ap√≥s corre√ß√£o:
- Rodar `python peab.py` novamente
- Selecionar `pima_indians_diabetes`
- Verificar que TODAS rejeitadas est√£o na zona

### 4. Validar Corre√ß√£o

```bash
python verificar_normalizacao.py
```

Deve mostrar: `‚úì Todas as inst√¢ncias rejeitadas est√£o corretamente na zona!`

---

## üìä Impacto Esperado Ap√≥s Corre√ß√£o

### Antes (Atual)
- 44 inst√¢ncias marcadas como rejeitadas
- Apenas 20 realmente na zona (45%)
- Fidelidade: ~23%

### Depois (Esperado)
- ~20 inst√¢ncias rejeitadas (apenas as que est√£o na zona)
- 100% na zona (por defini√ß√£o)
- Fidelidade: **~70-90%** (muito mais alta!)

### Por Qu√™?
Com as inst√¢ncias corretas:
- Elas **realmente** est√£o na zona amb√≠gua
- Explica√ß√µes do PEAB s√£o adequadas
- Perturba√ß√µes mant√™m inst√¢ncias na zona
- Fidelidade alta como esperado

---

## üéØ Valida√ß√£o da Hip√≥tese Original

**Sua pergunta estava 100% correta:**
> "Nas rejeitadas o que se espera √© que mesmo perturbando ainda continuem na zona de rejei√ß√£o?"

**SIM!** E o problema n√£o era com o conceito, mas com a **implementa√ß√£o**:
- Inst√¢ncias marcadas como rejeitadas N√ÉO est√£o na zona
- Por isso n√£o conseguem "continuar" na zona (nunca estiveram!)
- Ap√≥s corre√ß√£o, a fidelidade deve subir significativamente

---

## üìÅ Arquivos de Investiga√ß√£o

Criados para diagn√≥stico:
- `investigar_rejeitadas.py` - An√°lise de perturba√ß√µes
- `verificar_normalizacao.py` - Verifica√ß√£o de consist√™ncia

Execute ap√≥s corre√ß√£o para validar!

---

## ‚ú® Pr√≥ximos Passos

1. ‚òê Corrigir c√≥digo de classifica√ß√£o em `peab.py`
2. ‚òê Re-executar PEAB em `pima_indians_diabetes`
3. ‚òê Validar com `verificar_normalizacao.py`
4. ‚òê Re-executar valida√ß√£o: `python peab_validation.py`
5. ‚òê Esperar fidelidade ~70-90% para rejeitadas
6. ‚òê Repetir para outros datasets

---

**Status:** ‚ö†Ô∏è AGUARDANDO CORRE√á√ÉO DO BUG

*Documento gerado em: 12 de dezembro de 2025*
