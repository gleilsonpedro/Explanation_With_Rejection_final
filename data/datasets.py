# datasets.py (MODIFICADO)
import os
import io
import zipfile
import time
import re
import requests
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris, fetch_openml, load_breast_cancer, load_wine
from typing import List, Tuple, Dict, Any, Optional

# ------------------------ Utilitﾃ｡rios de download/cache ------------------------
def _ensure_dir(path: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)


def _download_with_retries(url: str, dest_path: str, max_retries: int = 3, timeout: int = 20) -> bool:
    """Tenta baixar um arquivo com cabeﾃｧalho User-Agent e backoff exponencial.
    Retorna True se salvou em dest_path, False caso contrﾃ｡rio.
    """
    session = requests.Session()
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
        "Accept": "text/csv,application/octet-stream,*/*;q=0.8",
        "Connection": "keep-alive",
    }

    backoff = 1.5
    delay = 1.0
    for attempt in range(1, max_retries + 1):
        try:
            resp = session.get(url, headers=headers, timeout=timeout)
            # Respeita 429 Too Many Requests com pequeno backoff
            if resp.status_code == 429:
                retry_after = resp.headers.get("Retry-After")
                if retry_after and retry_after.isdigit():
                    wait_s = min(int(retry_after), 10)
                else:
                    wait_s = min(int(delay), 10)
                time.sleep(wait_s)
                delay *= backoff
                continue

            if 200 <= resp.status_code < 300:
                _ensure_dir(dest_path)
                with open(dest_path, "wb") as f:
                    f.write(resp.content)
                return True
            # Em outros cﾃｳdigos, tenta novamente com backoff leve
            time.sleep(min(int(delay), 5))
            delay *= backoff
        except requests.RequestException:
            time.sleep(min(int(delay), 5))
            delay *= backoff
    return False


def _read_csv_with_cache(urls: List[str], local_path: str, **read_csv_kwargs) -> pd.DataFrame:
    """Lﾃｪ CSV de um caminho local (se existir) ou tenta baixar de uma lista de URLs.
    Em caso de falha no download, levanta uma exceﾃｧﾃ｣o com instruﾃｧﾃｵes para cache manual.
    """
    # 1) Usa cache local se existir
    if os.path.exists(local_path):
        return pd.read_csv(local_path, **read_csv_kwargs)

    # 2) Tenta baixar de alguma URL
    for url in urls:
        if _download_with_retries(url, local_path):
            try:
                return pd.read_csv(local_path, **read_csv_kwargs)
            except Exception:
                # Arquivo baixado pode nﾃ｣o estar conforme esperado; tenta prﾃｳxima URL
                try:
                    os.remove(local_path)
                except OSError:
                    pass
                continue

    # 3) Falhou totalmente
    raise RuntimeError(
        (
            f"Nﾃ｣o foi possﾃｭvel obter o dataset a partir das URLs fornecidas. "
            f"Por favor, baixe manualmente o arquivo e salve em '{local_path}'."
        )
    )


def _fetch_openml_with_retries(dataset_name: str, version: int = 1, as_frame: bool = True, max_retries: int = 3):
    """Wrapper para fetch_openml com tentativas e backoff simples."""
    delay = 1.0
    backoff = 1.7
    last_err = None
    for _ in range(max_retries):
        try:
            return fetch_openml(dataset_name, version=version, as_frame=as_frame, parser='auto')
        except Exception as e:
            last_err = e
            time.sleep(min(int(delay), 5))
            delay *= backoff
    raise last_err if last_err else RuntimeError(f"Falha ao baixar dataset OpenML: {dataset_name}")

RANDOM_STATE: int = 42

# ------------------------ Opﾃｧﾃｵes globais especﾃｭficas do MNIST ------------------------
# Modo de features para MNIST: 'raw' (784) ou 'pool2x2' (196)
MNIST_FEATURE_MODE: str = 'raw'
# Par de dﾃｭgitos selecionados (classe A vs classe B). None => usar todas as classes originais
MNIST_SELECTED_PAIR: Optional[Tuple[int, int]] = None


def set_mnist_options(feature_mode: str, pair: Optional[Tuple[int, int]]):
    """Define opﾃｧﾃｵes globais de MNIST que serﾃ｣o respeitadas por carregar_dataset.
    feature_mode: 'raw' ou 'pool2x2'; pair: (dA, dB) ou None.
    """
    global MNIST_FEATURE_MODE, MNIST_SELECTED_PAIR
    if feature_mode not in ('raw', 'pool2x2'):
        feature_mode = 'raw'
    MNIST_FEATURE_MODE = feature_mode
    MNIST_SELECTED_PAIR = pair


def _pool2x2(arr28: np.ndarray) -> np.ndarray:
    """Aplica pooling 2x2 por mﾃｩdia em uma imagem 28x28, resultando em 14x14 (196)."""
    out = np.zeros((14, 14), dtype=float)
    for r in range(14):
        for c in range(14):
            block = arr28[2*r:2*r+2, 2*c:2*c+2]
            out[r, c] = float(block.mean())
    return out


def _apply_pooling_df(X: pd.DataFrame) -> pd.DataFrame:
    """Converte DataFrame com 784 colunas (28x28) para 196 (14x14) via pooling 2x2."""
    # cria mapeamento de ordem de colunas para 28x28
    # assume colunas ordenadas por ﾃｭndice (feature_0..feature_783 ou similares)
    arr = X.values.astype(float)
    n = arr.shape[0]
    pooled = np.zeros((n, 14*14), dtype=float)
    for i in range(n):
        img = arr[i].reshape(28, 28)
        pooled[i] = _pool2x2(img).reshape(-1)
    cols = [f"bin_{r}_{c}" for r in range(14) for c in range(14)]
    return pd.DataFrame(pooled, index=X.index, columns=cols)

# --- Funﾃｧﾃｵes de Carregamento e Preparaﾃｧﾃ｣o de Dataset ---
def carregar_dataset(nome_dataset: str) -> Tuple[Optional[pd.DataFrame], Optional[pd.Series], Optional[List[str]]]: #
    try:
        X, y_series, class_names_list = None, None, None
        # --- Iris Dataset (Classes 0 e 1 apenas) ---
        if nome_dataset == 'iris':
            data = load_iris()
            X = pd.DataFrame(data.data, columns=data.feature_names)
            y_series = pd.Series(data.target, name='target')
            class_names_list = list(data.target_names)

            # Binarizaﾃｧﾃ｣o OvR (classe 0 vs resto)
            y_series = np.where(y_series == 0, 0, 1)  # Setosa (0) vs Versicolor +      Virginica (1)
            class_names_list = [class_names_list[0], "Nﾃ｣o-Setosa"]  # ["setosa",        "Nﾃ｣o-Setosa"]
        elif nome_dataset == 'sonar':
            urls = [
                "https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data",
            ]
            local_path = os.path.join('data', 'sonar.all-data')
            # As features nﾃ｣o tﾃｪm nomes, entﾃ｣o vamos nomeﾃ｡-las genericamente
            col_names = [f"feature_{i}" for i in range(60)] + ["target"]
            data_df = _read_csv_with_cache(urls, local_path, header=None, names=col_names)
            # Converte a classe alvo (M para mina, R para rocha) em 1 e 0
            data_df["target"] = data_df["target"].apply(lambda x: 1 if x == 'M' else 0)
            X = data_df.drop("target", axis=1)
            y_series = data_df["target"]
            class_names_list = ["Rocha", "Mina (Metal)"]
        elif nome_dataset == 'pima_indians_diabetes':
            # URLs alternativas para reduzir risco de 429 e quedas de serviﾃｧo
            urls = [
                "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv",
                "https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data",
            ]
            local_path = os.path.join('data', 'pima-indians-diabetes.csv')
            col_names = ['num_gravidezes', 'glicose', 'pressao_sangue', 'espessura_pele',
                         'insulina', 'imc', 'diabetes_pedigree', 'idade', 'target']
            # Usa cache local se existir; caso contrﾃ｡rio, tenta baixar de uma das URLs
            data_df = _read_csv_with_cache(urls, local_path, header=None, names=col_names)
            
            # --- SEﾃﾃグ DE TRATAMENTO REMOVIDA PARA COMPARAﾃﾃグ DIRETA COM O DO MATEUS ---
            # As linhas abaixo, que limpavam os zeros invﾃ｡lidos, foram removidas.
            # colunas_com_zero_invalido = ['glicose', 'pressao_sangue', 'espessura_pele', 'insulina', 'imc']
            # data_df[colunas_com_zero_invalido] = data_df[colunas_com_zero_invalido].replace(0, np.nan)
            # data_df.dropna(inplace=True)
            # data_df.reset_index(drop=True, inplace=True)
            # --- FIM DA SEﾃﾃグ REMOVIDA ---

            X = data_df.drop('target', axis=1)
            y_series = data_df['target'].astype(int)
            class_names_list = ['Nﾃ｣o Diabﾃｩtico', 'Diabﾃｩtico']
            
        elif nome_dataset == 'breast_cancer':
            data = load_breast_cancer()
            X = pd.DataFrame(data.data, columns=data.feature_names)
            y_series = pd.Series(data.target, name='target')
            class_names_list = list(data.target_names)
        elif nome_dataset == 'banknote':
            urls = [
                "https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt"
            ]
            local_path = os.path.join('data', 'data_banknote_authentication.txt')
            col_names = ["variance", "skewness", "curtosis", "entropy", "target"]
            data_df = _read_csv_with_cache(urls, local_path, names=col_names)
            X = data_df.drop("target", axis=1)
            y_series = data_df["target"].astype(int)
            class_names_list = ["Autﾃｪntica", "Falsificada"]
        elif nome_dataset == 'heart_disease':
            urls = [
                "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"
            ]
            col_names = ["age", "sex", "cp", "trestbps", "chol", "fbs", "restecg",
                         "thalach", "exang", "oldpeak", "slope", "ca", "thal", "target"]
            local_path = os.path.join('data', 'processed.cleveland.data')
            data_df = _read_csv_with_cache(urls, local_path, names=col_names, na_values="?")
            data_df.dropna(inplace=True)
            data_df["target"] = pd.to_numeric(data_df["target"], errors='coerce')
            data_df.dropna(subset=["target"], inplace=True)
            data_df["target"] = data_df["target"].astype(int).apply(lambda x: 1 if x > 0 else 0)
            X = data_df.drop("target", axis=1).astype(float)
            y_series = data_df["target"]
            class_names_list = ["Sem Doenﾃｧa", "Com Doenﾃｧa"]
        elif nome_dataset == 'wine_quality':
            urls = [
                "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
            ]
            local_path = os.path.join('data', 'winequality-red.csv')
            data_df = _read_csv_with_cache(urls, local_path, sep=";")
            data_df["target"] = data_df["quality"].apply(lambda x: 1 if x >= 7 else 0)
            X = data_df.drop(["quality", "target"], axis=1)
            y_series = data_df["target"]
            class_names_list = ["Baixa Qualidade", "Alta Qualidade"]
        elif nome_dataset == 'creditcard':
            # Apenas carrega o dataset completo e o retorna.
            # A amostragem serﾃ｡ feita no script principal.
            data_openml = _fetch_openml_with_retries('creditcard', version=1, as_frame=True)
            X = data_openml.data
            y_series = data_openml.target.astype(int)
            class_names_list = ['Normal', 'Fraude']
        elif nome_dataset == 'wine':
            data = load_wine()
            X = pd.DataFrame(data.data, columns=data.feature_names)
            y_series = pd.Series(data.target, name='target')
            class_names_list = list(data.target_names)
            
            # Binarizaﾃｧﾃ｣o OvR (classe 0 vs resto)
            y_series = np.where(y_series == 0, 0, 1)  # class_0 (0) vs class_1 + class_2        (1)
            class_names_list = [class_names_list[0], "Nﾃ｣o-class_0"]  # ["class_0",      "Nﾃ｣o-class_0"]
        elif nome_dataset == 'vertebral_column':
            url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00212/vertebral_column_data.zip"
            col_names = ["pelvic_incidence", "pelvic_tilt", "lumbar_lordosis_angle", 
                         "sacral_slope", "pelvic_radius", "spondylolisthesis_grade", "target"]
            file_path = os.path.join('data', 'column_2C.dat')
            # Faz download e extrai se nﾃ｣o existir localmente
            if not os.path.exists(file_path):
                zip_path = os.path.join('data', 'vertebral_column_data.zip')
                ok = _download_with_retries(url, zip_path)
                if not ok:
                    raise RuntimeError("Falha ao baixar vertebral_column_data.zip. Faﾃｧa o download manual e coloque em 'data/'.")
                with zipfile.ZipFile(zip_path, 'r') as zf:
                    # Extrai apenas o arquivo 2 classes (column_2C.dat)
                    for name in zf.namelist():
                        if name.endswith('column_2C.dat'):
                            zf.extract(name, 'data')
                            # Mover/renomear se estiver em subpasta
                            src = os.path.join('data', name)
                            if src != file_path:
                                os.replace(src, file_path)
                            break
            data_df = pd.read_csv(file_path, header=None, names=col_names, sep=r'\s+')
            data_df["target"] = data_df["target"].apply(lambda x: 0 if x == 'NO' else 1)
            X = data_df.drop("target", axis=1)
            y_series = data_df["target"]
            class_names_list = ["Normal", "Anormal"]
        elif nome_dataset == 'spambase':
            urls = [
                "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
            ]
            local_path = os.path.join('data', 'spambase.data')
            
            # Nomes das 57 features, seguidos pela coluna 'target'
            # (Conforme a documentaﾃｧﾃ｣o oficial do dataset)
            col_names = [
                "word_freq_make", "word_freq_address", "word_freq_all", "word_freq_3d", 
                "word_freq_our", "word_freq_over", "word_freq_remove", "word_freq_internet", 
                "word_freq_order", "word_freq_mail", "word_freq_receive", "word_freq_will", 
                "word_freq_people", "word_freq_report", "word_freq_addresses", "word_freq_free", 
                "word_freq_business", "word_freq_email", "word_freq_you", "word_freq_credit", 
                "word_freq_your", "word_freq_font", "word_freq_000", "word_freq_money", 
                "word_freq_hp", "word_freq_hpl", "word_freq_george", "word_freq_650", 
                "word_freq_lab", "word_freq_labs", "word_freq_telnet", "word_freq_857", 
                "word_freq_data", "word_freq_415", "word_freq_85", "word_freq_technology", 
                "word_freq_1999", "word_freq_parts", "word_freq_pm", "word_freq_direct", 
                "word_freq_cs", "word_freq_meeting", "word_freq_original", "word_freq_project", 
                "word_freq_re", "word_freq_edu", "word_freq_table", "word_freq_conference", 
                "char_freq_semicolon", "char_freq_parentheses", "char_freq_bracket", 
                "char_freq_exclamation", "char_freq_dollar", "char_freq_hash", 
                "capital_run_length_average", "capital_run_length_longest", 
                "capital_run_length_total", "target"
            ]
            
            # Carrega os dados usando pandas
            data_df = _read_csv_with_cache(urls, local_path, names=col_names, header=None)
            
            # Separa os alvos (y) e as features (X)
            y_series = data_df["target"].astype(int) # Alvo jﾃ｡ ﾃｩ 0 (nﾃ｣o spam) ou 1 (spam)
            X = data_df.drop("target", axis=1)
            
            # Define os nomes das classes para o menu
            class_names_list = ["Nﾃ｣o Spam", "Spam"]
        elif nome_dataset == 'mnist':
            # MNIST via OpenML (10 classes), com cache do scikit-learn e retries
            data_openml = _fetch_openml_with_retries('mnist_784', version=1, as_frame=True)
            X = data_openml.data
            y_all = pd.Series(data_openml.target.astype(int), name='target')
            class_names_list = [str(d) for d in range(10)]

            # Aplicar seleﾃｧﾃ｣o de par (classe vs classe) se definida globalmente
            global MNIST_SELECTED_PAIR, MNIST_FEATURE_MODE
            if MNIST_SELECTED_PAIR is not None:
                a, b = MNIST_SELECTED_PAIR
                mask = (y_all == a) | (y_all == b)
                X = X[mask].copy()
                y_bin_np = np.where(y_all[mask] == a, 0, 1)
                y_series = pd.Series(y_bin_np, index=X.index, name='target')
                class_names_list = [str(a), str(b)]
            else:
                # Sem par definido: retorna multiclasse original
                y_series = y_all

            # Aplicar pooling 2x2 (196 features) se requisitado
            if MNIST_FEATURE_MODE == 'pool2x2':
                X = _apply_pooling_df(X)
        else:
            raise ValueError(f"Dataset '{nome_dataset}' nﾃ｣o suportado.")

        if not isinstance(X, pd.DataFrame): X = pd.DataFrame(X)
        if not isinstance(y_series, pd.Series): y_series = pd.Series(y_series, name='target')
        
        if not hasattr(X.columns, 'tolist') or not X.columns.tolist() or all(isinstance(c, int) for c in X.columns):
            X.columns = [f"feature_{i}" for i in range(X.shape[1])]
        return X, y_series, class_names_list
    except Exception as e:
        print(f"\nErro ao carregar o dataset '{nome_dataset}': {str(e)}")
        return None, None, None

def selecionar_dataset_e_classe() -> Tuple[Optional[str], Optional[str], Optional[pd.DataFrame], Optional[pd.Series], Optional[List[str]]]:
    # [MODIFICAﾃﾃグ IMPORTANTE] Menu atualizado para incluir Iris e reorganizar as opﾃｧﾃｵes.
    menu = '''
    | ******************* MENU DE DATASETS DO EXPERIMENTO ****************** |
    | Datasets Clﾃ｡ssicos para Comparaﾃｧﾃ｣o:                                  |
    | [0] Breast Cancer (569x30x2)       | [1] MNIST (70k x 784 x 10)
    | [2] Pima Diabetes (392x8x2)        | [3] Sonar (208x60x2)            |
    | [4] Vertebral Column (310x6x2)     | [5] Wine (178x13x3)             |
    |----------------------------------------------------------------------|
    | Outros Datasets Disponﾃｭveis:                                         |
    | [6] Banknote Auth (1372x4x2)       | [7] Heart Disease (297x13x2)    |
    | [8] Wine Quality (Red) (1599x11x2) | [9] Spambase (210x7x3)             |
    | [10] Creditcard Fraud (Amostra)                                      |
    |----------------------------------------------------------------------|
    | [Q] SAIR                                                             |
    |----------------------------------------------------------------------|
    '''
    print(menu)
    # [MODIFICAﾃﾃグ IMPORTANTE] Lista de datasets atualizada para corresponder ao menu.
    nomes_datasets = [
        'breast_cancer', 'mnist', 'pima_indians_diabetes', 'sonar', 
        'vertebral_column', 'wine', 'banknote', 'heart_disease',
        'wine_quality', 'spambase', 'creditcard'
    ]
    while True:
        opcao = input("\nDigite o nﾃｺmero do dataset ou 'Q' para sair: ").upper().strip()
        if opcao == 'Q': return None, None, None, None, None

        if opcao.isdigit() and 0 <= int(opcao) < len(nomes_datasets):
            nome_dataset_selecionado = nomes_datasets[int(opcao)]
            print(f"\nCarregando {nome_dataset_selecionado}...")
            # Para MNIST, as configuraﾃｧﾃｵes sﾃ｣o automﾃ｡ticas (definidas em peab_2.MNIST_CONFIG)
            # Nﾃ｣o hﾃ｡ menu interativo - tudo ﾃｩ configurado via cﾃｳdigo
            if nome_dataset_selecionado == 'mnist':
                # Importar e aplicar configuraﾃｧﾃｵes de MNIST_CONFIG
                try:
                    import sys
                    import os
                    # Adicionar diretﾃｳrio raiz ao path se necessﾃ｡rio
                    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                    if root_dir not in sys.path:
                        sys.path.insert(0, root_dir)
                    
                    try:
                        from peab import MNIST_CONFIG
                        print("\n[INFO] MNIST detectado. Usando configuraﾃｧﾃｵes automﾃ｡ticas de peab.MNIST_CONFIG")
                    except ImportError:
                        try:
                            from peab_2 import MNIST_CONFIG
                            print("\n[INFO] MNIST detectado. Usando configuraﾃｧﾃｵes automﾃ｡ticas de peab_2.MNIST_CONFIG")
                        except ImportError:
                            raise ImportError("Nﾃ｣o foi possﾃｭvel importar MNIST_CONFIG de peab.py nem de peab_2.py")
                    
                    # Aplicar configuraﾃｧﾃｵes ANTES de carregar
                    feature_mode = MNIST_CONFIG.get('feature_mode', 'raw')
                    digit_pair = MNIST_CONFIG.get('digit_pair', None)
                    
                    if digit_pair is None:
                        print("[ERRO] MNIST_CONFIG['digit_pair'] nﾃ｣o estﾃ｡ definido!")
                        return None, None, None, None, None
                    
                    print(f"[INFO] Configuraﾃｧﾃｵes: feature_mode='{feature_mode}', digit_pair={digit_pair}")
                    
                    # Configurar opﾃｧﾃｵes globais ANTES de carregar
                    set_mnist_options(feature_mode, digit_pair)
                    
                except ImportError as e:
                    print(f"[ERRO] Falha na importaﾃｧﾃ｣o de configuraﾃｧﾃ｣o MNIST: {e}")
                    print("[INFO] Usando configuraﾃｧﾃ｣o padrﾃ｣o: raw, (3, 8)")
                    set_mnist_options('raw', (3, 8))
                
                # Agora carregar o dataset com as opﾃｧﾃｵes jﾃ｡ configuradas
                X_original_completo, y_original_completo, classes_originais_nomes = carregar_dataset('mnist')
            else:
                X_original_completo, y_original_completo, classes_originais_nomes = carregar_dataset(nome_dataset_selecionado)

            if X_original_completo is None or y_original_completo is None or classes_originais_nomes is None:
                print("Falha ao carregar dataset. Tente novamente.")
                continue

            print(f"Dataset carregado! (Total Amostras: {X_original_completo.shape[0]}, Features: {X_original_completo.shape[1]})")
            
            # Para MNIST, se as opﾃｧﾃｵes globais jﾃ｡ estﾃ｣o configuradas (par selecionado), Nﾃグ pedir menu
            if nome_dataset_selecionado == 'mnist' and MNIST_SELECTED_PAIR is not None:
                print(f"\n[INFO] Par de classes jﾃ｡ configurado automaticamente: {MNIST_SELECTED_PAIR[0]} vs {MNIST_SELECTED_PAIR[1]}")
                print("[INFO] Dataset MNIST jﾃ｡ filtrado para classificaﾃｧﾃ｣o binﾃ｡ria.")
                # O dataset jﾃ｡ foi carregado filtrado em carregar_dataset()
                # Retornar diretamente
                return nome_dataset_selecionado, str(MNIST_SELECTED_PAIR[0]), X_original_completo, y_original_completo, classes_originais_nomes
            
            print("\nClasses disponﾃｭveis no dataset original:")
            for i, nome_classe in enumerate(classes_originais_nomes):
                print(f"   [{i}] - {nome_classe} (Total: {sum(y_original_completo == i)})")

            # Se o dataset sﾃｳ tiver 2 classes, automatiza a seleﾃｧﾃ｣o
            if len(classes_originais_nomes) == 2:
                print("\nDataset binﾃ｡rio detectado. Configurando automaticamente Classe 0 vs Classe 1.")
                indice_classe_0 = 0
                indice_classe_1 = 1
                nome_classe_0_final = classes_originais_nomes[indice_classe_0]

            # Se tiver mais de 2 classes, pede ao usuﾃ｡rio para escolher
            else:
                indice_classe_0 = -1
                while True:
                    entrada_classe_0 = input(f"\nDigite o Nﾃ哺ERO da classe a ser CLASSE 0 (0-{len(classes_originais_nomes)-1}): ").strip()
                    if entrada_classe_0.isdigit() and 0 <= int(entrada_classe_0) < len(classes_originais_nomes):
                        indice_classe_0 = int(entrada_classe_0)
                        break
                    else:
                        print("Nﾃｺmero invﾃ｡lido!")
                
                nome_classe_0_final = classes_originais_nomes[indice_classe_0]
                indice_classe_1 = -1

                print(f"\nClasses restantes para CLASSE 1 (nﾃ｣o pode ser '{nome_classe_0_final}'):")
                indices_disponiveis_classe_1 = [i for i, _ in enumerate(classes_originais_nomes) if i != indice_classe_0]
                for i_disp in indices_disponiveis_classe_1:
                    print(f"   [{i_disp}] - {classes_originais_nomes[i_disp]} (Total: {sum(y_original_completo == i_disp)})")
                
                while True:
                    entrada_classe_1 = input(f"\nDigite o Nﾃ哺ERO da CLASSE 1: ").strip()
                    if entrada_classe_1.isdigit() and int(entrada_classe_1) in indices_disponiveis_classe_1:
                        indice_classe_1 = int(entrada_classe_1)
                        break
                    else:
                        print("Nﾃｺmero invﾃ｡lido ou classe jﾃ｡ escolhida!")
            
            # Filtragem e Mapeamento para Binﾃ｡rio
            mascara_classe_0 = (y_original_completo == indice_classe_0)
            mascara_classe_1 = (y_original_completo == indice_classe_1)
            mascara_combinada = mascara_classe_0 | mascara_classe_1

            X_filtrado = X_original_completo[mascara_combinada].copy()
            y_filtrado = y_original_completo[mascara_combinada].copy()

            # Para MNIST, pooling jﾃ｡ foi aplicado em carregar_dataset conforme opﾃｧﾃ｣o global
            
            # Mapeia para 0 e 1 (LogisticRegression prefere isso)
            y_binario_np = np.where(y_filtrado == indice_classe_0, 0, 1)
            y_binario = pd.Series(y_binario_np, index=X_filtrado.index, name='target_binario')
            nomes_classes_binarias_finais = [classes_originais_nomes[indice_classe_0], classes_originais_nomes[indice_classe_1]]

            print(f"\n隼 Dataset '{nome_dataset_selecionado}' configurado para classificaﾃｧﾃ｣o binﾃ｡ria:")
            print(f"   Classe 0 mapeada para: '{nomes_classes_binarias_finais[0]}' (Originalmente ﾃｭndice {indice_classe_0})")
            print(f"   Classe 1 mapeada para: '{nomes_classes_binarias_finais[1]}' (Originalmente ﾃｭndice {indice_classe_1})")
            print(f"   Total de amostras para o problema binﾃ｡rio: {X_filtrado.shape[0]}\n")

            return nome_dataset_selecionado, nome_classe_0_final, X_filtrado, y_binario, nomes_classes_binarias_finais
        else:
            print("Opﾃｧﾃ｣o invﾃ｡lida.")