"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    VALIDAÃ‡ÃƒO DE EXPLICAÃ‡Ã•ES - XAI COM REJEIÃ‡ÃƒO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Script de validaÃ§Ã£o rigorosa para mÃ©todos de explicaÃ§Ã£o (PEAB, PuLP, Anchor, MinExp).

Testa:
    - Fidelity (Fidelidade): % de perturbaÃ§Ãµes que mantÃªm prediÃ§Ã£o
    - Sufficiency (SuficiÃªncia): Apenas features da explicaÃ§Ã£o sÃ£o suficientes
    - Necessity (Necessidade): Cada feature Ã© necessÃ¡ria (nÃ£o redundante)
    - Stability (Estabilidade): ExplicaÃ§Ã£o Ã© determinÃ­stica
    - Coverage (Cobertura): % de instÃ¢ncias sem erro/timeout

Autor: Sistema de ValidaÃ§Ã£o XAI
Data: Dezembro 2025
"""

import numpy as np
import pandas as pd
import json
import os
import time
import warnings
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns

# Suprimir warnings
warnings.filterwarnings("ignore")

# Configurar estilo dos plots
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

# Constantes
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURAÃ‡ÃƒO DE PERTURBAÃ‡Ã•ES (PADRÃƒO FIXO PARA COMPARAÃ‡ÃƒO JUSTA)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NÃºmero de perturbaÃ§Ãµes por instÃ¢ncia (recomendado: 500-1000)
# Valores testados academicamente: 100, 500, 1000, 2000
# AUMENTAR para datasets pequenos (mais estatÃ­stico)
# DIMINUIR para datasets grandes (MNIST, CIFAR) por questÃ£o de tempo
NUM_PERTURBATIONS_DEFAULT = 1000  # PadrÃ£o para datasets normais (< 500 features)
NUM_PERTURBATIONS_LARGE = 500     # Para datasets grandes (>= 500 features ou > 1000 instÃ¢ncias)

# EstratÃ©gia de perturbaÃ§Ã£o (FIXO: uniforme Ã© o padrÃ£o acadÃªmico)
# OpÃ§Ãµes: 'uniform', 'distribution', 'adversarial'
# RECOMENDADO: 'uniform' (testa todo o espaÃ§o, mais rigoroso)
PERTURBATION_STRATEGY = "uniform"
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Paths
JSON_DIR = "json"
VALIDATION_JSON_DIR = os.path.join(JSON_DIR, "validation")
RESULTS_DIR = "results"
VALIDATION_RESULTS_DIR = os.path.join(RESULTS_DIR, "validation")

# Criar diretÃ³rios se nÃ£o existirem
os.makedirs(VALIDATION_JSON_DIR, exist_ok=True)
os.makedirs(VALIDATION_RESULTS_DIR, exist_ok=True)


def encontrar_variacao_mnist(metodo: str) -> Optional[str]:
    """
    Busca por variaÃ§Ãµes de MNIST disponÃ­veis (mnist_3_vs_6.json, mnist_1_vs_2.json, etc).
    
    Args:
        metodo: Nome do mÃ©todo ('PEAB', 'PuLP', etc)
    
    Returns:
        Nome do dataset encontrado ou None
    """
    metodo_lower = metodo.lower()
    metodo_dir = os.path.join(JSON_DIR, metodo_lower)
    
    if not os.path.exists(metodo_dir):
        return None
    
    # Procura por arquivos que comeÃ§am com "mnist"
    mnist_files = [f for f in os.listdir(metodo_dir) if f.startswith('mnist') and f.endswith('.json')]
    
    if not mnist_files:
        return None
    
    # Se houver apenas 1, retorna automaticamente
    if len(mnist_files) == 1:
        dataset_name = mnist_files[0].replace('.json', '')
        print(f"\nâœ“ MNIST encontrado: {dataset_name}")
        return dataset_name
    
    # Se houver mÃºltiplas, mostra menu
    print("\nğŸ” MÃºltiplas variaÃ§Ãµes de MNIST encontradas:")
    print("â”€" * 60)
    for i, f in enumerate(mnist_files, 1):
        dataset_name = f.replace('.json', '')
        print(f"  {i}. {dataset_name}")
    
    print("â”€" * 60)
    escolha = input("Qual variaÃ§Ã£o deseja usar? (nÃºmero): ").strip()
    
    try:
        idx = int(escolha) - 1
        if 0 <= idx < len(mnist_files):
            dataset_name = mnist_files[idx].replace('.json', '')
            return dataset_name
        else:
            print("âŒ OpÃ§Ã£o invÃ¡lida")
            return None
    except ValueError:
        print("âŒ Digite um nÃºmero vÃ¡lido")
        return None


def carregar_resultados_metodo(metodo: str, dataset: str) -> Optional[Tuple]:
    """
    Carrega os resultados de execuÃ§Ã£o de um mÃ©todo (PEAB, PuLP, Anchor, MinExp).
    
    NOVA ESTRUTURA: Carrega de json/{method}/{dataset}.json
    
    Suporta busca automÃ¡tica de variaÃ§Ãµes MNIST se dataset nÃ£o for encontrado.
    
    Args:
        metodo: Nome do mÃ©todo ('PEAB', 'PuLP', 'Anchor', 'MinExp')
        dataset: Nome do dataset
    
    Returns:
        Tupla (dados, dataset_usado) onde dataset_usado pode ser diferente
        de dataset (ex: mnist_3_vs_6 em vez de mnist)
        Retorna None se nÃ£o encontrado
    """
    metodo_lower = metodo.lower()
    if metodo_lower == 'pulp':
        metodo_lower = 'pulp'  # PuLP usa 'pulp' como nome de pasta
    
    # Nova estrutura: json/{method}/{dataset}.json
    json_path = os.path.join(JSON_DIR, metodo_lower, f"{dataset}.json")
    dataset_usado = dataset
    
    # Se nÃ£o encontrar e for mnist, procura por variaÃ§Ãµes
    if not os.path.exists(json_path) and dataset == 'mnist':
        print(f"\nâš  {dataset}.json nÃ£o encontrado em json/{metodo_lower}/")
        print("  Procurando por variaÃ§Ãµes de MNIST...")
        dataset_encontrado = encontrar_variacao_mnist(metodo)
        
        if dataset_encontrado:
            json_path = os.path.join(JSON_DIR, metodo_lower, f"{dataset_encontrado}.json")
            dataset_usado = dataset_encontrado
        else:
            print(f"âŒ Nenhuma variaÃ§Ã£o de MNIST encontrada em json/{metodo_lower}/")
            return None
    
    if not os.path.exists(json_path):
        print(f"âŒ Arquivo nÃ£o encontrado: {json_path}")
        print(f"   Execute primeiro: python {metodo_lower}.py")
        print(f"   E selecione o dataset: {dataset}")
        return None
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Retorna tupla com dados e dataset usado
        return (data, dataset_usado)
    
    except Exception as e:
        print(f"âŒ Erro ao carregar {json_path}: {e}")
        return None


def carregar_pipeline_dataset(dataset: str):
    """
    Carrega o pipeline treinado e dados do dataset usando shared_training.
    Detecta variaÃ§Ãµes MNIST (mnist_3_vs_8, mnist_1_vs_2, etc) e configura
    o par automÃ¡ticamente.
    
    Args:
        dataset: Nome do dataset (pode ser 'mnist', 'mnist_3_vs_8', etc)
    
    Returns:
        Tupla (pipeline, X_train, X_test, y_train, y_test, t_plus, t_minus, meta)
    """
    try:
        from utils.shared_training import get_shared_pipeline
        from data.datasets import set_mnist_options
        import re
        
        # Detecta variaÃ§Ãµes MNIST e extrai o par (ex: mnist_3_vs_8 -> (3, 8))
        if dataset.startswith('mnist_') and '_vs_' in dataset:
            match = re.match(r'mnist_(\d+)_vs_(\d+)', dataset)
            if match:
                digit_a, digit_b = int(match.group(1)), int(match.group(2))
                set_mnist_options('raw', (digit_a, digit_b))
                dataset_to_load = 'mnist'
            else:
                dataset_to_load = dataset
        else:
            dataset_to_load = dataset
        
        return get_shared_pipeline(dataset_to_load)
    except Exception as e:
        print(f"âŒ Erro ao carregar pipeline: {e}")
        return None


def gerar_perturbacoes(
    instancia_original: np.ndarray,
    features_fixas: List[int],
    X_train: pd.DataFrame,
    n_perturbacoes: int = 1000,
    estrategia: str = "uniform"
) -> np.ndarray:
    """
    Gera perturbaÃ§Ãµes de uma instÃ¢ncia fixando features da explicaÃ§Ã£o.
    
    Args:
        instancia_original: InstÃ¢ncia original (vetor de features)
        features_fixas: Ãndices das features da explicaÃ§Ã£o (fixar valores)
        X_train: Dados de treino (para distribuiÃ§Ã£o)
        n_perturbacoes: NÃºmero de perturbaÃ§Ãµes a gerar
        estrategia: 'uniform', 'distribution', ou 'adversarial'
    
    Returns:
        Array (n_perturbacoes, n_features) com perturbaÃ§Ãµes
    """
    n_features = len(instancia_original)
    perturbacoes = np.tile(instancia_original, (n_perturbacoes, 1))
    
    # Features que serÃ£o perturbadas (nÃ£o estÃ£o na explicaÃ§Ã£o)
    features_perturbar = [i for i in range(n_features) if i not in features_fixas]
    
    if len(features_perturbar) == 0:
        # ExplicaÃ§Ã£o usa todas as features, nada a perturbar
        return perturbacoes
    
    # Obter valores min/max do dataset
    X_train_arr = X_train.values if hasattr(X_train, 'values') else X_train
    
    for feat_idx in features_perturbar:
        feat_min = X_train_arr[:, feat_idx].min()
        feat_max = X_train_arr[:, feat_idx].max()
        
        if estrategia == "uniform":
            # Valores aleatÃ³rios uniformes [min, max]
            perturbacoes[:, feat_idx] = np.random.uniform(feat_min, feat_max, n_perturbacoes)
        
        elif estrategia == "distribution":
            # Sample da distribuiÃ§Ã£o real do treino
            perturbacoes[:, feat_idx] = np.random.choice(
                X_train_arr[:, feat_idx], 
                size=n_perturbacoes, 
                replace=True
            )
        
        elif estrategia == "adversarial":
            # Valores extremos (50% min, 50% max)
            n_min = n_perturbacoes // 2
            perturbacoes[:n_min, feat_idx] = feat_min
            perturbacoes[n_min:, feat_idx] = feat_max
    
    return perturbacoes


def validar_fidelity_instancia(
    instancia_idx: int,
    explicacao_features: List[str],
    feature_names: List[str],
    y_true: int,
    y_pred: int,
    rejeitada: bool,
    pipeline,
    X_test: pd.DataFrame,
    X_train: pd.DataFrame,
    t_plus: float,
    t_minus: float,
    n_perturbacoes: int = 1000,
    estrategia: str = "uniform"
) -> Dict:
    """
    Valida fidelity de uma instÃ¢ncia usando perturbaÃ§Ãµes.
    
    Returns:
        Dict com mÃ©tricas: fidelity, sufficiency, perturbations_tested, etc.
    """
    # Obter instÃ¢ncia original
    # Tentar primeiro usar label-based indexing (.loc), depois position-based (.iloc)
    try:
        # Tentar como label do Ã­ndice (que Ã© o que PEAB salva no JSON)
        instancia_original = X_test.loc[instancia_idx].values
    except (KeyError, TypeError):
        try:
            # Se falhar, tentar como Ã­ndice posicional (posiÃ§Ã£o)
            instancia_original = X_test.iloc[int(instancia_idx)].values
        except (IndexError, ValueError):
            # Se ainda falhar, logar erro e retornar None
            return {
                'fidelity': -1,
                'sufficiency': -1,
                'explanation_size': len(explicacao_features),
                'perturbations_tested': 0,
                'error': f"NÃ£o foi possÃ­vel acessar instÃ¢ncia {instancia_idx} em X_test"
            }
    
    # Mapear nomes de features para Ã­ndices
    features_fixas_idx = [feature_names.index(feat) for feat in explicacao_features if feat in feature_names]
    
    # Gerar perturbaÃ§Ãµes
    perturbacoes = gerar_perturbacoes(
        instancia_original,
        features_fixas_idx,
        X_train,
        n_perturbacoes,
        estrategia
    )
    
    # Reclassificar perturbaÃ§Ãµes
    try:
        predicoes = pipeline.predict(perturbacoes)
        scores = pipeline.decision_function(perturbacoes)
    except Exception as e:
        print(f"âš ï¸  Erro ao reclassificar instÃ¢ncia {instancia_idx}: {e}")
        return {
            'fidelity': 0.0,
            'sufficiency': 0.0,
            'perturbations_tested': 0,
            'perturbations_correct': 0,
            'error': str(e)
        }
    
    # Contar acertos baseado no tipo de prediÃ§Ã£o original
    if rejeitada:
        # InstÃ¢ncia rejeitada: todas as perturbaÃ§Ãµes devem cair na zona de rejeiÃ§Ã£o
        acertos = np.sum((scores >= t_minus) & (scores <= t_plus))
    else:
        # InstÃ¢ncia aceita: perturbaÃ§Ãµes devem ter mesma classe
        acertos = np.sum(predicoes == y_pred)
    
    fidelity = (acertos / n_perturbacoes) * 100.0
    
    return {
        'fidelity': float(fidelity),
        'sufficiency': float(fidelity),  # Para mÃ©todos Ã³timos, suficiÃªncia = fidelity
        'perturbations_tested': int(n_perturbacoes),
        'perturbations_correct': int(acertos)
    }


def validar_metodo(
    metodo: str,
    dataset: str,
    n_perturbacoes: int = None,
    estrategia: str = None,
    verbose: bool = True
) -> Dict:
    """
    Valida um mÃ©todo completo (PEAB, PuLP, Anchor, MinExp).
    
    Args:
        metodo: Nome do mÃ©todo
        dataset: Nome do dataset
        n_perturbacoes: NÃºmero de perturbaÃ§Ãµes (None = usar padrÃ£o automÃ¡tico)
        estrategia: EstratÃ©gia de perturbaÃ§Ã£o (None = usar PERTURBATION_STRATEGY)
        verbose: Mostrar progresso
    
    Returns:
        DicionÃ¡rio com todas as mÃ©tricas de validaÃ§Ã£o
    """
    # Carregar resultados do mÃ©todo (retorna tupla com dataset correto)
    resultado_carga = carregar_resultados_metodo(metodo, dataset)
    if resultado_carga is None:
        return None
    
    resultados, dataset_correto = resultado_carga
    
    if verbose:
        print(f"\n{'â•'*70}")
        print(f"  VALIDANDO: {metodo.upper()} - Dataset: {dataset_correto}")
        print(f"{'â•'*70}")
    
    # Carregar pipeline e dados (dataset_correto jÃ¡ contÃ©m MNIST_X_vs_Y se necessÃ¡rio)
    pipeline_data = carregar_pipeline_dataset(dataset_correto)
    if pipeline_data is None:
        return None
    
    pipeline, X_train, X_test, y_train, y_test, t_plus, t_minus, meta = pipeline_data
    feature_names = meta['feature_names']
    
    # Determinar nÃºmero de perturbaÃ§Ãµes automaticamente se nÃ£o especificado
    if n_perturbacoes is None:
        num_features = len(feature_names)
        num_instances = len(X_test)
        
        # Ajuste automÃ¡tico: datasets grandes â†’ menos perturbaÃ§Ãµes
        if num_features >= 500 or num_instances > 1000:
            n_perturbacoes = NUM_PERTURBATIONS_LARGE
            if verbose:
                print(f"[AUTO] Dataset grande detectado: usando {n_perturbacoes} perturbaÃ§Ãµes")
        else:
            n_perturbacoes = NUM_PERTURBATIONS_DEFAULT
    
    # Usar estratÃ©gia padrÃ£o se nÃ£o especificada
    if estrategia is None:
        estrategia = PERTURBATION_STRATEGY
    
    # Obter explicaÃ§Ãµes do JSON
    explicacoes = resultados.get('explicacoes', resultados.get('per_instance', []))
    
    if not explicacoes:
        print(f"\nâŒ ERRO: Nenhuma explicaÃ§Ã£o individual encontrada em {metodo}_results.json")
        print(f"\n{'â”€'*70}")
        print("SOLUÃ‡ÃƒO:")
        print(f"  1. O arquivo {metodo.lower()}_results.json existe mas NÃƒO contÃ©m explicaÃ§Ãµes individuais")
        print(f"  2. Execute novamente o mÃ©todo para gerar explicaÃ§Ãµes completas:")
        print(f"     python {metodo.lower()}.py")
        print(f"  3. Selecione o dataset: {dataset}")
        print(f"\n  NOTA: O JSON atual contÃ©m apenas estatÃ­sticas agregadas.")
        print(f"        A validaÃ§Ã£o precisa das explicaÃ§Ãµes individuais (por instÃ¢ncia).")
        print(f"{'â”€'*70}\n")
        return None
    
    if verbose:
        print(f"â†’ Validando {len(explicacoes)} explicaÃ§Ãµes...")
        print(f"â†’ PerturbaÃ§Ãµes por instÃ¢ncia: {n_perturbacoes}")
        print(f"â†’ EstratÃ©gia: {estrategia}")
    
    # Inicializar mÃ©tricas
    metricas_por_instancia = []
    tamanhos_explicacao = []
    fidelities = []
    
    # MÃ©tricas por tipo
    metricas_por_tipo = {
        'positive': {'fidelities': [], 'sizes': [], 'count': 0},
        'negative': {'fidelities': [], 'sizes': [], 'count': 0},
        'rejected': {'fidelities': [], 'sizes': [], 'count': 0}
    }
    
    # DistribuiÃ§Ã£o de tamanhos
    size_distribution = defaultdict(int)
    
    # Tempo de inÃ­cio
    start_time = time.time()
    
    # Validar cada explicaÃ§Ã£o
    from utils.progress_bar import ProgressBar
    
    with ProgressBar(total=len(explicacoes), description=f"Validando {metodo}") as pbar:
        for exp in explicacoes:
            idx = exp.get('indice', exp.get('id'))
            if idx is None:
                pbar.update()
                continue
            
            idx = int(idx)
            
            # Extrair informaÃ§Ãµes da explicaÃ§Ã£o - suporta ambos os formatos
            # Formato novo: 'explanation' + 'explanation_size'
            # Formato antigo: 'explicacao' ou 'features'
            if 'explanation' in exp:
                explicacao_features = exp['explanation']
                tamanho = exp.get('explanation_size', len(explicacao_features))
            elif 'explicacao' in exp:
                explicacao_features = exp['explicacao']
                tamanho = len(explicacao_features)
            elif 'features' in exp:
                explicacao_features = exp['features']
                tamanho = len(explicacao_features)
            else:
                pbar.update()
                continue
            
            tamanhos_explicacao.append(tamanho)
            
            # Contar distribuiÃ§Ã£o de tamanhos
            if tamanho >= 6:
                size_distribution['6+'] += 1
            else:
                size_distribution[str(tamanho)] += 1
            
            y_true = int(exp.get('y_true', exp.get('classe_real', -1)))
            y_pred = int(exp.get('y_pred', exp.get('predicao', -1)))
            # Suporta ambos os formatos: 'rejected' (booleano) ou 'rejeitada' (booleano)
            rejeitada = bool(exp.get('rejected', exp.get('rejeitada', False)))
            
            # Determinar tipo: se rejected=True, Ã© rejeitada (mesmo que y_pred seja -1)
            if rejeitada:
                tipo = 'rejected'
            elif y_pred == 1:
                tipo = 'positive'
            elif y_pred == 0:
                tipo = 'negative'
            else:
                # Se y_pred for -1 ou outro valor invÃ¡lido
                tipo = 'rejected'
            
            # Validar fidelity
            resultado = validar_fidelity_instancia(
                idx,
                explicacao_features,
                feature_names,
                y_true,
                y_pred,
                rejeitada,
                pipeline,
                X_test,
                X_train,
                t_plus,
                t_minus,
                n_perturbacoes,
                estrategia
            )
            
            fidelity = resultado['fidelity']
            
            # Se houver erro ao processar a instÃ¢ncia, pular
            if 'error' in resultado:
                pbar.update()
                continue
            
            fidelities.append(fidelity)
            
            # Atualizar mÃ©tricas por tipo
            metricas_por_tipo[tipo]['fidelities'].append(fidelity)
            metricas_por_tipo[tipo]['sizes'].append(tamanho)
            metricas_por_tipo[tipo]['count'] += 1
            
            # Armazenar resultado
            metricas_por_instancia.append({
                'instance_id': idx,
                'y_true': y_true,
                'y_pred': y_pred,
                'rejected': rejeitada,
                'explanation_size': tamanho,
                'explanation_features': explicacao_features,
                'fidelity': fidelity,
                'sufficiency': resultado['sufficiency'],
                'perturbations_tested': resultado['perturbations_tested'],
                'perturbations_correct': resultado['perturbations_correct']
            })
            
            pbar.update()
    
    # Calcular tempo total
    validation_time = time.time() - start_time
    
    # Calcular mÃ©tricas globais
    fidelity_overall = np.mean(fidelities)
    
    # Calcular mÃ©tricas por tipo
    per_type_metrics = {}
    for tipo, dados in metricas_por_tipo.items():
        if dados['count'] > 0:
            per_type_metrics[tipo] = {
                'count': dados['count'],
                'fidelity': float(np.mean(dados['fidelities'])),
                'mean_size': float(np.mean(dados['sizes'])),
                'std_size': float(np.std(dados['sizes']))
            }
        else:
            per_type_metrics[tipo] = {
                'count': 0,
                'fidelity': 0.0,
                'mean_size': 0.0,
                'std_size': 0.0
            }
    
    # Calcular reduction rate
    num_features = len(feature_names)
    mean_size = np.mean(tamanhos_explicacao)
    reduction_rate = ((num_features - mean_size) / num_features) * 100.0
    
    # Montar resultado final
    resultado_validacao = {
        'metadata': {
            'method': metodo,
            'dataset': dataset,
            'date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'num_perturbations': n_perturbacoes,
            'perturbation_strategy': estrategia,
            'test_instances': len(explicacoes),
            'num_features': num_features
        },
        'global_metrics': {
            'fidelity_overall': float(fidelity_overall),
            'fidelity_positive': float(per_type_metrics['positive']['fidelity']),
            'fidelity_negative': float(per_type_metrics['negative']['fidelity']),
            'fidelity_rejected': float(per_type_metrics['rejected']['fidelity']),
            'sufficiency': float(fidelity_overall),  # Para mÃ©todos Ã³timos
            'coverage': 100.0,  # % instÃ¢ncias sem erro
            'mean_explanation_size': float(mean_size),
            'median_explanation_size': float(np.median(tamanhos_explicacao)),
            'std_explanation_size': float(np.std(tamanhos_explicacao)),
            'min_explanation_size': int(np.min(tamanhos_explicacao)),
            'max_explanation_size': int(np.max(tamanhos_explicacao)),
            'reduction_rate': float(reduction_rate),
            'validation_time_seconds': float(validation_time)
        },
        'per_type_metrics': per_type_metrics,
        'size_distribution': dict(size_distribution),
        'per_instance_results': metricas_por_instancia
    }
    
    if verbose:
        print(f"\nâœ“ ValidaÃ§Ã£o completa!")
        print(f"  - Fidelity Geral: {fidelity_overall:.2f}%")
        print(f"  - Tamanho MÃ©dio: {mean_size:.2f}")
        print(f"  - Taxa de ReduÃ§Ã£o: {reduction_rate:.2f}%")
        print(f"  - Tempo: {validation_time:.2f}s")
    
    return resultado_validacao


def salvar_json_validacao(resultado: Dict, metodo: str, dataset: str):
    """Salva resultado da validaÃ§Ã£o em JSON."""
    json_path = os.path.join(VALIDATION_JSON_DIR, f"{metodo.lower()}_validation_{dataset}.json")
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(resultado, f, indent=2, ensure_ascii=False)
    
    print(f"âœ“ JSON salvo: {json_path}")


def gerar_relatorio_txt(resultado: Dict, metodo: str, dataset: str):
    """Gera relatÃ³rio TXT profissional adequado para dissertaÃ§Ã£o."""
    
    output_dir = os.path.join(VALIDATION_RESULTS_DIR, dataset, metodo.lower())
    os.makedirs(output_dir, exist_ok=True)
    report_path = os.path.join(output_dir, "validation_report.txt")
    
    meta = resultado['metadata']
    globais = resultado['global_metrics']
    por_tipo = resultado['per_type_metrics']
    dist_size = resultado['size_distribution']
    
    # Converter nome do dataset para display
    dataset_display = dataset.replace('_', ' ').title()
    metodo_display = metodo.upper()
    
    with open(report_path, 'w', encoding='utf-8') as f:
        # CabeÃ§alho
        f.write("â•”" + "â•" * 78 + "â•—\n")
        f.write("â•‘" + " " * 78 + "â•‘\n")
        f.write("â•‘" + f"RELATÃ“RIO DE VALIDAÃ‡ÃƒO DE EXPLICABILIDADE - MÃ‰TODO {metodo_display}".center(78) + "â•‘\n")
        f.write("â•‘" + f"Dataset: {dataset_display}".center(78) + "â•‘\n")
        f.write("â•‘" + " " * 78 + "â•‘\n")
        f.write("â•š" + "â•" * 78 + "â•\n\n")
        
        # SEÃ‡ÃƒO 1: DescriÃ§Ã£o do MÃ©todo
        f.write("â”" * 80 + "\n")
        f.write("1. DESCRIÃ‡ÃƒO DO MÃ‰TODO DE VALIDAÃ‡ÃƒO\n")
        f.write("â”" * 80 + "\n\n")
        f.write("Este relatÃ³rio apresenta a validaÃ§Ã£o da qualidade das explicaÃ§Ãµes geradas\n")
        f.write("pelo mÃ©todo de Explainability AI (Explicabilidade em InteligÃªncia Artificial).\n\n")
        f.write(f"MÃ‰TODO UTILIZADO: {metodo_display}\n")
        f.write("TÃ‰CNICA DE VALIDAÃ‡ÃƒO: AvaliaÃ§Ã£o de Fidelidade por PerturbaÃ§Ã£o\n\n")
        f.write("A fidelidade Ã© medida atravÃ©s de perturbaÃ§Ãµes nos dados de entrada:\n")
        f.write(f"  â€¢ {meta['num_perturbations']:,} variaÃ§Ãµes foram aplicadas a cada instÃ¢ncia\n")
        f.write("  â€¢ Cada variaÃ§Ã£o altera os valores das features de forma sistemÃ¡tica\n")
        f.write("  â€¢ Verifica-se se a prediÃ§Ã£o do modelo permanece a mesma com as\n")
        f.write("    features explicativas em seus valores perturbados\n")
        f.write("  â€¢ Uma alta taxa de consistÃªncia indica que a explicaÃ§Ã£o Ã© fiel ao\n")
        f.write("    comportamento real do modelo (alta fidelidade)\n\n")
        f.write("ESTRATÃ‰GIA DE PERTURBAÃ‡ÃƒO: Uniforme\n")
        f.write("  â€¢ Valores das features sÃ£o aleatoriamente substituÃ­dos dentro de seus\n")
        f.write("    intervalos observados (mÃ­nimo-mÃ¡ximo) no conjunto de treinamento\n")
        f.write("  â€¢ Essa abordagem rigorosa testa o mÃ©todo em cenÃ¡rios variados\n\n")
        f.write("â”" * 80 + "\n\n")
        
        # SEÃ‡ÃƒO 2: ConfiguraÃ§Ã£o do Experimento
        f.write("â”" * 80 + "\n")
        f.write("2. CONFIGURAÃ‡ÃƒO DO EXPERIMENTO\n")
        f.write("â”" * 80 + "\n\n")
        f.write(f"  Base de Dados:                    {dataset_display}\n")
        f.write(f"  InstÃ¢ncias Validadas:             {meta['test_instances']} amostras\n")
        f.write(f"  NÃºmero de VariÃ¡veis (Features):   {meta['num_features']}\n")
        f.write(f"  PerturbaÃ§Ãµes por InstÃ¢ncia:       {meta['num_perturbations']:,}\n")
        f.write(f"  Total de AvaliaÃ§Ãµes:              {meta['test_instances'] * meta['num_perturbations']:,}\n")
        f.write(f"  Data de ExecuÃ§Ã£o:                 {meta['date']}\n\n")
        f.write("â”" * 80 + "\n\n")
        
        # SEÃ‡ÃƒO 3: Resultados Principais
        f.write("â”" * 80 + "\n")
        f.write("3. RESULTADOS PRINCIPAIS\n")
        f.write("â”" * 80 + "\n\n")
        
        f.write("3.1 FIDELIDADE DAS EXPLICAÃ‡Ã•ES\n")
        f.write("â”€" * 80 + "\n\n")
        f.write(f"  Fidelidade Geral:                 {globais['fidelity_overall']:.2f}%\n\n")
        
        f.write("  Fidelidade por Tipo de PrediÃ§Ã£o:\n")
        for tipo_nome, tipo_label, emoji in [('positive', 'PrediÃ§Ãµes Positivas', 'â—‹'), 
                                               ('negative', 'PrediÃ§Ãµes Negativas', 'â—'), 
                                               ('rejected', 'PrediÃ§Ãµes Rejeitadas', 'â—†')]:
            dados = por_tipo[tipo_nome]
            f.write(f"    {emoji} {tipo_label:.<40} {dados['fidelity']:>6.2f}% ({dados['count']:>3} instÃ¢ncias)\n")
        
        f.write(f"\n  Taxa de Cobertura (sem erros):    {globais['coverage']:.2f}%\n")
        f.write(f"  InstÃ¢ncias Processadas com Sucesso: {int(globais['coverage'] / 100 * meta['test_instances'])} / {meta['test_instances']}\n")
        f.write("\n")
        
        f.write("3.2 CARACTERÃSTICAS DAS EXPLICAÃ‡Ã•ES\n")
        f.write("â”€" * 80 + "\n\n")
        f.write("  Tamanho das ExplicaÃ§Ãµes (nÃºmero de variÃ¡veis selecionadas):\n")
        f.write(f"    â€¢ MÃ©dia:                        {globais['mean_explanation_size']:.2f} variÃ¡veis\n")
        f.write(f"    â€¢ Mediana:                      {globais['median_explanation_size']:.0f} variÃ¡veis\n")
        f.write(f"    â€¢ Desvio PadrÃ£o:                {globais['std_explanation_size']:.2f}\n")
        f.write(f"    â€¢ Intervalo:                    {globais['min_explanation_size']} a {globais['max_explanation_size']} variÃ¡veis\n")
        f.write(f"    â€¢ Taxa de CompactaÃ§Ã£o:          {globais['reduction_rate']:.1f}%\n")
        f.write(f"      (reduÃ§Ã£o em relaÃ§Ã£o ao total de {meta['num_features']} variÃ¡veis)\n")
        f.write("\n")
        
        f.write("3.3 DISTRIBUIÃ‡ÃƒO DE TAMANHOS DAS EXPLICAÃ‡Ã•ES\n")
        f.write("â”€" * 80 + "\n\n")
        f.write("  VariÃ¡veis â”‚ Quantidade â”‚ Porcentagem â”‚ VisualizaÃ§Ã£o\n")
        f.write("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼" + "â”€" * 42 + "\n")
        
        total = meta['test_instances']
        for size in sorted(dist_size.keys(), key=lambda x: int(x.replace('+', '')) if x != '6+' else 6):
            count = dist_size[size]
            pct = (count / total) * 100
            bar_len = int(pct / 2)
            bar = "â–ˆ" * bar_len
            f.write(f"     {size:>4}    â”‚    {count:>4}    â”‚    {pct:>5.1f}%   â”‚ {bar:<40}\n")
        f.write("\n")
        
        # SEÃ‡ÃƒO 4: AnÃ¡lise Detalhada
        f.write("â”" * 80 + "\n")
        f.write("4. ANÃLISE DETALHADA POR TIPO DE PREDIÃ‡ÃƒO\n")
        f.write("â”" * 80 + "\n\n")
        
        tipos_info = [
            ('positive', 'PrediÃ§Ãµes Positivas', 'InstÃ¢ncias classificadas como positivas pelo modelo', 'A'),
            ('negative', 'PrediÃ§Ãµes Negativas', 'InstÃ¢ncias classificadas como negativas pelo modelo', 'B'),
            ('rejected', 'PrediÃ§Ãµes Rejeitadas', 'InstÃ¢ncias onde o modelo aplicou mecanismo de rejeiÃ§Ã£o', 'C')
        ]
        
        for tipo_nome, tipo_label, descricao, idx in tipos_info:
            dados = por_tipo[tipo_nome]
            f.write(f"4.{idx} {tipo_label.upper()}\n")
            f.write("â”€" * 80 + "\n")
            f.write(f"    DescriÃ§Ã£o: {descricao}\n\n")
            f.write(f"    Quantidade de InstÃ¢ncias:       {dados['count']} ({dados['count']/total*100:.1f}%)\n")
            f.write(f"    Fidelidade MÃ©dio:               {dados['fidelity']:.2f}%\n")
            f.write(f"    Tamanho MÃ©dio da ExplicaÃ§Ã£o:    {dados['mean_size']:.2f} variÃ¡veis\n")
            f.write(f"    Desvio PadrÃ£o do Tamanho:       {dados['std_size']:.2f}\n\n")
        
        f.write("â”" * 80 + "\n\n")
        
        # SEÃ‡ÃƒO 5: InterpretaÃ§Ã£o dos Resultados
        f.write("â”" * 80 + "\n")
        f.write("5. INTERPRETAÃ‡ÃƒO E CONCLUSÃ•ES\n")
        f.write("â”" * 80 + "\n\n")
        
        # AnÃ¡lise de Fidelidade
        if globais['fidelity_overall'] >= 99.0:
            conclusao_fidelidade = "Excelente"
            texto_fidelidade = "O mÃ©todo produz explicaÃ§Ãµes de qualidade excepcional."
        elif globais['fidelity_overall'] >= 95.0:
            conclusao_fidelidade = "Muito Boa"
            texto_fidelidade = "As explicaÃ§Ãµes apresentam alta fidelidade ao comportamento do modelo."
        elif globais['fidelity_overall'] >= 85.0:
            conclusao_fidelidade = "Boa"
            texto_fidelidade = "As explicaÃ§Ãµes sÃ£o geralmente confiÃ¡veis."
        elif globais['fidelity_overall'] >= 75.0:
            conclusao_fidelidade = "AceitÃ¡vel"
            texto_fidelidade = "As explicaÃ§Ãµes apresentam qualidade aceitÃ¡vel."
        else:
            conclusao_fidelidade = "Requer RevisÃ£o"
            texto_fidelidade = "As explicaÃ§Ãµes devem ser analisadas criticamente."
        
        f.write(f"FIDELIDADE: {conclusao_fidelidade}\n")
        f.write(f"  {texto_fidelidade}\n")
        f.write(f"  Com uma fidelidade de {globais['fidelity_overall']:.2f}%, as explicaÃ§Ãµes geradas\n")
        f.write(f"  mantÃªm consistÃªncia em {globais['fidelity_overall']:.2f}% dos cenÃ¡rios testados quando\n")
        f.write(f"  as features nÃ£o selecionadas sÃ£o aleatoriamente perturbadas.\n\n")
        
        # AnÃ¡lise de CompactaÃ§Ã£o
        f.write(f"COMPACTAÃ‡ÃƒO: {100 - globais['reduction_rate']:.1f}% das Features NecessÃ¡rias\n")
        f.write(f"  As explicaÃ§Ãµes utilizam em mÃ©dia apenas {globais['mean_explanation_size']:.2f} de {meta['num_features']} variÃ¡veis,\n")
        f.write(f"  representando uma reduÃ§Ã£o de {globais['reduction_rate']:.1f}%.\n")
        f.write(f"  Isso torna as explicaÃ§Ãµes bastante compactas e fÃ¡ceis de interpretar.\n\n")
        
        # AnÃ¡lise de Cobertura
        if globais['coverage'] == 100.0:
            f.write(f"COBERTURA: Completa (100%)\n")
            f.write(f"  Todas as {meta['test_instances']} instÃ¢ncias foram processadas com sucesso,\n")
            f.write(f"  sem erros ou timeouts durante a validaÃ§Ã£o.\n\n")
        else:
            f.write(f"COBERTURA: {globais['coverage']:.2f}%\n")
            f.write(f"  {int(globais['coverage'] / 100 * meta['test_instances'])} de {meta['test_instances']} instÃ¢ncias foram processadas com sucesso.\n")
            f.write(f"  {100 - globais['coverage']:.2f}% das instÃ¢ncias apresentaram erros ou timeouts.\n\n")
        
        f.write("â”" * 80 + "\n\n")
        
        # SEÃ‡ÃƒO 6: RecomendaÃ§Ãµes
        f.write("â”" * 80 + "\n")
        f.write("6. RECOMENDAÃ‡Ã•ES\n")
        f.write("â”" * 80 + "\n\n")
        
        if globais['fidelity_overall'] >= 95.0:
            f.write("  âœ“ O mÃ©todo estÃ¡ validado e pronto para uso.\n")
            f.write("  âœ“ As explicaÃ§Ãµes podem ser confiÃ¡veis e utilizadas em aplicaÃ§Ãµes prÃ¡ticas.\n")
        else:
            f.write("  â€¢ Verificar configuraÃ§Ãµes de hiperparÃ¢metros do mÃ©todo.\n")
            f.write("  â€¢ Revisar instÃ¢ncias com baixa fidelidade para identificar padrÃµes.\n")
            f.write("  â€¢ Considerar ajustes na estratÃ©gia de seleÃ§Ã£o de features.\n")
        
        f.write("\n")
        f.write("â”" * 80 + "\n")
        f.write(f"RelatÃ³rio gerado em: {meta['date']}\n")
        f.write("â”" * 80 + "\n")
    
    print(f"âœ“ RelatÃ³rio salvo: {report_path}")
    return report_path


def gerar_plots(resultado: Dict, metodo: str, dataset: str):
    """Gera os 6 plots de validaÃ§Ã£o."""
    
    output_dir = os.path.join(VALIDATION_RESULTS_DIR, dataset, metodo.lower())
    os.makedirs(output_dir, exist_ok=True)
    
    globais = resultado['global_metrics']
    por_tipo = resultado['per_type_metrics']
    per_instance = resultado['per_instance_results']
    
    # Extrair dados
    sizes = [inst['explanation_size'] for inst in per_instance]
    fidelities = [inst['fidelity'] for inst in per_instance]
    tipos = []
    for inst in per_instance:
        if inst['rejected']:
            tipos.append('Rejeitada')
        elif inst['y_pred'] == 1:
            tipos.append('Positiva')
        else:
            tipos.append('Negativa')
    
    # Plot 1: Histograma de Fidelity
    plt.figure(figsize=(10, 6))
    plt.hist(fidelities, bins=20, edgecolor='black', alpha=0.7)
    plt.xlabel('Fidelity (%)', fontsize=12)
    plt.ylabel('FrequÃªncia', fontsize=12)
    plt.title(f'DistribuiÃ§Ã£o de Fidelity - {metodo.upper()} ({dataset})', fontsize=14, fontweight='bold')
    plt.axvline(globais['fidelity_overall'], color='red', linestyle='--', linewidth=2, label=f'MÃ©dia: {globais["fidelity_overall"]:.2f}%')
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_fidelity_histogram.png'), dpi=300)
    plt.close()
    
    # Plot 2: Boxplot de Tamanhos
    plt.figure(figsize=(8, 6))
    plt.boxplot(sizes, vert=True, patch_artist=True,
                boxprops=dict(facecolor='lightblue', alpha=0.7),
                medianprops=dict(color='red', linewidth=2))
    plt.ylabel('Tamanho da ExplicaÃ§Ã£o', fontsize=12)
    plt.title(f'DistribuiÃ§Ã£o de Tamanhos - {metodo.upper()} ({dataset})', fontsize=14, fontweight='bold')
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_boxplot_sizes.png'), dpi=300)
    plt.close()
    
    # Plot 3: Scatter Tamanho vs Fidelity
    plt.figure(figsize=(10, 6))
    cores = {'Positiva': 'green', 'Negativa': 'red', 'Rejeitada': 'orange'}
    for tipo in ['Positiva', 'Negativa', 'Rejeitada']:
        mask = [t == tipo for t in tipos]
        sizes_tipo = [s for s, m in zip(sizes, mask) if m]
        fid_tipo = [f for f, m in zip(fidelities, mask) if m]
        plt.scatter(sizes_tipo, fid_tipo, alpha=0.6, s=50, label=tipo, color=cores[tipo])
    
    plt.xlabel('Tamanho da ExplicaÃ§Ã£o', fontsize=12)
    plt.ylabel('Fidelity (%)', fontsize=12)
    plt.title(f'Tamanho vs Fidelity - {metodo.upper()} ({dataset})', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_size_vs_fidelity.png'), dpi=300)
    plt.close()
    
    # Plot 4: Heatmap Fidelity por Tipo
    fig, ax = plt.subplots(figsize=(8, 4))
    tipos_ordem = ['positive', 'negative', 'rejected']
    tipos_labels = ['Positiva', 'Negativa', 'Rejeitada']
    fidelities_por_tipo = [por_tipo[t]['fidelity'] for t in tipos_ordem]
    
    data_heatmap = np.array(fidelities_por_tipo).reshape(1, -1)
    sns.heatmap(data_heatmap, annot=True, fmt='.2f', cmap='RdYlGn', vmin=0, vmax=100,
                xticklabels=tipos_labels, yticklabels=[metodo], cbar_kws={'label': 'Fidelity (%)'})
    plt.title(f'Fidelity por Tipo de PrediÃ§Ã£o - {metodo.upper()} ({dataset})', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_heatmap_types.png'), dpi=300)
    plt.close()
    
    # Plot 5: Violin Plot de Tamanhos por Tipo
    plt.figure(figsize=(10, 6))
    df_plot = pd.DataFrame({'Tamanho': sizes, 'Tipo': tipos})
    ordem_tipos = ['Positiva', 'Negativa', 'Rejeitada']
    sns.violinplot(data=df_plot, x='Tipo', y='Tamanho', order=ordem_tipos, palette='Set2')
    plt.title(f'DistribuiÃ§Ã£o de Tamanhos por Tipo - {metodo.upper()} ({dataset})', fontsize=14, fontweight='bold')
    plt.ylabel('Tamanho da ExplicaÃ§Ã£o', fontsize=12)
    plt.xlabel('Tipo de PrediÃ§Ã£o', fontsize=12)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_violin_sizes.png'), dpi=300)
    plt.close()
    
    # Plot 6: MÃ©trica Reduction vs Fidelity
    plt.figure(figsize=(8, 6))
    reduction = globais['reduction_rate']
    fidelity = globais['fidelity_overall']
    
    plt.scatter([reduction], [fidelity], s=500, c='blue', alpha=0.6, edgecolors='black', linewidth=2)
    plt.annotate(metodo.upper(), (reduction, fidelity), fontsize=12, fontweight='bold',
                 xytext=(10, 10), textcoords='offset points')
    
    plt.xlabel('Taxa de ReduÃ§Ã£o (%)', fontsize=12)
    plt.ylabel('Fidelity (%)', fontsize=12)
    plt.title(f'EficiÃªncia: ReduÃ§Ã£o vs Fidelity - {metodo.upper()} ({dataset})', fontsize=14, fontweight='bold')
    plt.xlim(0, 100)
    plt.ylim(0, 105)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_reduction_vs_fidelity.png'), dpi=300)
    plt.close()
    
    print(f"âœ“ Plots salvos (6): {output_dir}/")


def menu_principal():
    """Menu interativo principal."""
    
    print("\n" + "â•" * 70)
    print("           VALIDAÃ‡ÃƒO DE EXPLICAÃ‡Ã•ES - XAI COM REJEIÃ‡ÃƒO")
    print("â•" * 70)
    print("\n[1] Validar PEAB")
    print("[2] Validar PuLP (Ground Truth)")
    print("[3] Validar Anchor")
    print("[4] Validar MinExp")
    print("[5] Comparar Todos os MÃ©todos (RECOMENDADO)")
    print("[0] Sair")
    
    opcao = input("\nOpÃ§Ã£o: ").strip()
    
    if opcao == '0':
        print("Encerrando...")
        return
    
    # Selecionar dataset (reutilizar menu do PEAB)
    print("\n" + "â”€" * 70)
    print("Selecione o dataset para validaÃ§Ã£o...")
    print("â”€" * 70)
    
    try:
        from data.datasets import selecionar_dataset_e_classe
        nome_dataset, _, _, _, _ = selecionar_dataset_e_classe()
        
        if nome_dataset is None:
            print("âŒ Nenhum dataset selecionado.")
            return
    
    except Exception as e:
        print(f"âŒ Erro ao carregar menu de datasets: {e}")
        return
    
    # ConfiguraÃ§Ã£o automÃ¡tica (sem menu)
    print("\n" + "â”€" * 70)
    print("CONFIGURAÃ‡ÃƒO DA VALIDAÃ‡ÃƒO")
    print("â”€" * 70)
    print(f"â†’ EstratÃ©gia: {PERTURBATION_STRATEGY.upper()} (padrÃ£o fixo)")
    print(f"â†’ PerturbaÃ§Ãµes: Ajuste automÃ¡tico por tamanho do dataset")
    print(f"   â€¢ Datasets normais: {NUM_PERTURBATIONS_DEFAULT} perturbaÃ§Ãµes/instÃ¢ncia")
    print(f"   â€¢ Datasets grandes: {NUM_PERTURBATIONS_LARGE} perturbaÃ§Ãµes/instÃ¢ncia")
    print("â”€" * 70)
    
    # Executar validaÃ§Ã£o (sem passar n_perturbacoes e estrategia â†’ usa padrÃµes)
    if opcao in ['1', '2', '3', '4']:
        metodos_map = {'1': 'PEAB', '2': 'PuLP', '3': 'Anchor', '4': 'MinExp'}
        metodo = metodos_map[opcao]
        
        resultado = validar_metodo(metodo, nome_dataset)  # Usa padrÃµes automÃ¡ticos
        
        if resultado:
            salvar_json_validacao(resultado, metodo, nome_dataset)
            gerar_relatorio_txt(resultado, metodo, nome_dataset)
            gerar_plots(resultado, metodo, nome_dataset)
            
            print("\n" + "â•" * 70)
            print("âœ“ VALIDAÃ‡ÃƒO COMPLETA!")
            print("â•" * 70)
    
    elif opcao == '5':
        print("\nâ†’ Validando todos os mÃ©todos...")
        
        metodos = ['PEAB', 'PuLP', 'Anchor', 'MinExp']
        resultados = {}
        
        for metodo in metodos:
            resultado = validar_metodo(metodo, nome_dataset)  # Usa padrÃµes automÃ¡ticos
            
            if resultado:
                resultados[metodo] = resultado
                salvar_json_validacao(resultado, metodo, nome_dataset)
                gerar_relatorio_txt(resultado, metodo, nome_dataset)
                gerar_plots(resultado, metodo, nome_dataset)
        
        # Gerar comparaÃ§Ã£o
        if len(resultados) > 1:
            print("\nâ†’ Gerando comparaÃ§Ã£o entre mÃ©todos...")
            gerar_comparacao(resultados, nome_dataset)
        
        print("\n" + "â•" * 70)
        print("âœ“ VALIDAÃ‡ÃƒO COMPLETA PARA TODOS OS MÃ‰TODOS!")
        print("â•" * 70)
    
    else:
        print("âŒ OpÃ§Ã£o invÃ¡lida.")


def gerar_comparacao(resultados: Dict[str, Dict], dataset: str):
    """Gera relatÃ³rio e plots comparando todos os mÃ©todos."""
    
    output_dir = os.path.join(VALIDATION_RESULTS_DIR, dataset, "comparison")
    os.makedirs(output_dir, exist_ok=True)
    
    # Criar tabela comparativa
    report_path = os.path.join(output_dir, "comparison_report.txt")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("â•" * 80 + "\n")
        f.write("        COMPARAÃ‡ÃƒO DE MÃ‰TODOS - VALIDAÃ‡ÃƒO DE EXPLICAÃ‡Ã•ES\n")
        f.write("â•" * 80 + "\n\n")
        
        f.write(f"Dataset: {dataset}\n")
        f.write(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # Tabela comparativa
        f.write("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n")
        f.write("â”‚ MÃ©trica         â”‚   PEAB   â”‚   PuLP   â”‚  Anchor  â”‚  MinExp  â”‚\n")
        f.write("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n")
        
        metricas_chaves = [
            ('fidelity_overall', 'Fidelity (%)'),
            ('mean_explanation_size', 'Tamanho MÃ©dio'),
            ('reduction_rate', 'ReduÃ§Ã£o (%)'),
            ('validation_time_seconds', 'Tempo (s)')
        ]
        
        for chave, label in metricas_chaves:
            valores = []
            for metodo in ['PEAB', 'PuLP', 'Anchor', 'MinExp']:
                if metodo in resultados:
                    val = resultados[metodo]['global_metrics'][chave]
                    valores.append(f"{val:>8.2f}")
                else:
                    valores.append("    N/A ")
            
            f.write(f"â”‚ {label:<15} â”‚ {valores[0]} â”‚ {valores[1]} â”‚ {valores[2]} â”‚ {valores[3]} â”‚\n")
        
        f.write("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n")
        
        # Ranking
        f.write("RANKING POR FIDELITY:\n")
        f.write("â”€" * 80 + "\n")
        
        ranking = sorted(resultados.items(), 
                        key=lambda x: x[1]['global_metrics']['fidelity_overall'],
                        reverse=True)
        
        for i, (metodo, res) in enumerate(ranking, 1):
            fid = res['global_metrics']['fidelity_overall']
            size = res['global_metrics']['mean_explanation_size']
            f.write(f"{i}. {metodo:<10} - Fidelity: {fid:.2f}% | Tamanho: {size:.2f}\n")
        
        f.write("â•" * 80 + "\n")
    
    print(f"âœ“ ComparaÃ§Ã£o salva: {report_path}")
    
    # Plot comparativo
    plt.figure(figsize=(12, 6))
    
    metodos_nomes = list(resultados.keys())
    fidelities = [resultados[m]['global_metrics']['fidelity_overall'] for m in metodos_nomes]
    sizes = [resultados[m]['global_metrics']['mean_explanation_size'] for m in metodos_nomes]
    
    x = np.arange(len(metodos_nomes))
    width = 0.35
    
    fig, ax1 = plt.subplots(figsize=(10, 6))
    
    ax1.bar(x - width/2, fidelities, width, label='Fidelity (%)', color='skyblue', edgecolor='black')
    ax1.set_ylabel('Fidelity (%)', fontsize=12)
    ax1.set_ylim(0, 105)
    
    ax2 = ax1.twinx()
    ax2.bar(x + width/2, sizes, width, label='Tamanho MÃ©dio', color='lightcoral', edgecolor='black')
    ax2.set_ylabel('Tamanho MÃ©dio da ExplicaÃ§Ã£o', fontsize=12)
    
    ax1.set_xlabel('MÃ©todo', fontsize=12)
    ax1.set_title(f'ComparaÃ§Ã£o de MÃ©todos - {dataset}', fontsize=14, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(metodos_nomes)
    ax1.legend(loc='upper left')
    ax2.legend(loc='upper right')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_methods_comparison.png'), dpi=300)
    plt.close()
    
    print(f"âœ“ Plot comparativo salvo: {output_dir}/")


if __name__ == '__main__':
    menu_principal()
