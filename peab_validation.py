"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    VALIDAÃ‡ÃƒO DE EXPLICAÃ‡Ã•ES - XAI COM REJEIÃ‡ÃƒO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Script de validaÃ§Ã£o rigorosa para mÃ©todos de explicaÃ§Ã£o (PEAB, PuLP, Anchor, MinExp).

Testa:
    - Fidelity (Fidelidade): % de perturbaÃ§Ãµes que mantÃªm prediÃ§Ã£o
    - Sufficiency (SuficiÃªncia): Apenas features da explicaÃ§Ã£o sÃ£o suficientes
    - Necessity (Necessidade): Cada feature Ã© necessÃ¡ria (nÃ£o redundante)
    - Stability (Estabilidade): ExplicaÃ§Ã£o Ã© determinÃ­stica
    - Coverage (Cobertura): % de instÃ¢ncias sem erro/timeout

Autor: Sistema de ValidaÃ§Ã£o XAI
Data: Dezembro 2025
"""

import numpy as np
import pandas as pd
import json
import os
import time
import warnings
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
import pulp  # [NOVO] Para validaÃ§Ã£o GLOBAL via LP solver (PuLP)

# Suprimir warnings
warnings.filterwarnings("ignore")

# Configurar estilo dos plots
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

# Constantes
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURAÃ‡ÃƒO DE PERTURBAÃ‡Ã•ES (PADRÃƒO FIXO PARA COMPARAÃ‡ÃƒO JUSTA)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NÃºmero de perturbaÃ§Ãµes por instÃ¢ncia (recomendado: 500-1000)
# Valores testados academicamente: 100, 500, 1000, 2000
# AUMENTAR para datasets pequenos (mais estatÃ­stico)
# DIMINUIR para datasets grandes (MNIST, CIFAR) por questÃ£o de tempo
NUM_PERTURBATIONS_DEFAULT = 1000  # PadrÃ£o para datasets normais (< 500 features)
NUM_PERTURBATIONS_LARGE = 500     # Para datasets grandes (>= 500 features ou > 1000 instÃ¢ncias)

# EstratÃ©gia de perturbaÃ§Ã£o (FIXO: uniforme Ã© o padrÃ£o acadÃªmico)
# OpÃ§Ãµes: 'uniform', 'distribution', 'adversarial'
# RECOMENDADO: 'uniform' (testa todo o espaÃ§o, mais rigoroso)
PERTURBATION_STRATEGY = "uniform"
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Paths
JSON_DIR = "json"
VALIDATION_JSON_DIR = os.path.join(JSON_DIR, "validation")
RESULTS_DIR = "results"
VALIDATION_RESULTS_DIR = os.path.join(RESULTS_DIR, "validation")

# Criar diretÃ³rios se nÃ£o existirem
os.makedirs(VALIDATION_JSON_DIR, exist_ok=True)
os.makedirs(VALIDATION_RESULTS_DIR, exist_ok=True)


def encontrar_variacao_mnist(metodo: str) -> Optional[str]:
    """
    Busca por variaÃ§Ãµes de MNIST disponÃ­veis (mnist_3_vs_6.json, mnist_1_vs_2.json, etc).
    
    Args:
        metodo: Nome do mÃ©todo ('PEAB', 'PuLP', etc)
    
    Returns:
        Nome do dataset encontrado ou None
    """
    metodo_lower = metodo.lower()
    metodo_dir = os.path.join(JSON_DIR, metodo_lower)
    
    if not os.path.exists(metodo_dir):
        return None
    
    # Procura por arquivos que comeÃ§am com "mnist"
    mnist_files = [f for f in os.listdir(metodo_dir) if f.startswith('mnist') and f.endswith('.json')]
    
    if not mnist_files:
        return None
    
    # Se houver apenas 1, retorna automaticamente
    if len(mnist_files) == 1:
        dataset_name = mnist_files[0].replace('.json', '')
        print(f"\nâœ“ MNIST encontrado: {dataset_name}")
        return dataset_name
    
    # Se houver mÃºltiplas, mostra menu
    print("\nğŸ” MÃºltiplas variaÃ§Ãµes de MNIST encontradas:")
    print("â”€" * 60)
    for i, f in enumerate(mnist_files, 1):
        dataset_name = f.replace('.json', '')
        print(f"  {i}. {dataset_name}")
    
    print("â”€" * 60)
    escolha = input("Qual variaÃ§Ã£o deseja usar? (nÃºmero): ").strip()
    
    try:
        idx = int(escolha) - 1
        if 0 <= idx < len(mnist_files):
            dataset_name = mnist_files[idx].replace('.json', '')
            return dataset_name
        else:
            print("âŒ OpÃ§Ã£o invÃ¡lida")
            return None
    except ValueError:
        print("âŒ Digite um nÃºmero vÃ¡lido")
        return None


def carregar_resultados_metodo(metodo: str, dataset: str) -> Optional[Tuple]:
    """
    Carrega os resultados de execuÃ§Ã£o de um mÃ©todo (PEAB, PuLP, Anchor, MinExp).
    
    NOVA ESTRUTURA: Carrega de json/{method}/{dataset}.json
    
    Suporta busca automÃ¡tica de variaÃ§Ãµes MNIST se dataset nÃ£o for encontrado.
    
    Args:
        metodo: Nome do mÃ©todo ('PEAB', 'PuLP', 'Anchor', 'MinExp')
        dataset: Nome do dataset
    
    Returns:
        Tupla (dados, dataset_usado) onde dataset_usado pode ser diferente
        de dataset (ex: mnist_3_vs_6 em vez de mnist)
        Retorna None se nÃ£o encontrado
    """
    metodo_lower = metodo.lower()
    if metodo_lower == 'pulp':
        metodo_lower = 'pulp'  # PuLP usa 'pulp' como nome de pasta
    
    # Nova estrutura: json/{method}/{dataset}.json
    json_path = os.path.join(JSON_DIR, metodo_lower, f"{dataset}.json")
    dataset_usado = dataset
    
    # Se nÃ£o encontrar e for mnist, procura por variaÃ§Ãµes
    if not os.path.exists(json_path) and dataset == 'mnist':
        print(f"\nâš  {dataset}.json nÃ£o encontrado em json/{metodo_lower}/")
        print("  Procurando por variaÃ§Ãµes de MNIST...")
        dataset_encontrado = encontrar_variacao_mnist(metodo)
        
        if dataset_encontrado:
            json_path = os.path.join(JSON_DIR, metodo_lower, f"{dataset_encontrado}.json")
            dataset_usado = dataset_encontrado
        else:
            print(f"âŒ Nenhuma variaÃ§Ã£o de MNIST encontrada em json/{metodo_lower}/")
            return None
    
    if not os.path.exists(json_path):
        print(f"âŒ Arquivo nÃ£o encontrado: {json_path}")
        print(f"   Execute primeiro: python {metodo_lower}.py")
        print(f"   E selecione o dataset: {dataset}")
        return None
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Retorna tupla com dados e dataset usado
        return (data, dataset_usado)
    
    except Exception as e:
        print(f"âŒ Erro ao carregar {json_path}: {e}")
        return None


def carregar_pipeline_dataset(dataset: str):
    """
    Carrega o pipeline treinado e dados do dataset usando shared_training.
    Detecta variaÃ§Ãµes MNIST (mnist_3_vs_8, mnist_1_vs_2, etc) e configura
    o par automÃ¡ticamente.
    
    Args:
        dataset: Nome do dataset (pode ser 'mnist', 'mnist_3_vs_8', etc)
    
    Returns:
        Tupla (pipeline, X_train, X_test, y_train, y_test, t_plus, t_minus, meta)
    """
    try:
        from utils.shared_training import get_shared_pipeline
        from data.datasets import set_mnist_options
        import re
        
        # Detecta variaÃ§Ãµes MNIST e extrai o par (ex: mnist_3_vs_8 -> (3, 8))
        if dataset.startswith('mnist_') and '_vs_' in dataset:
            match = re.match(r'mnist_(\d+)_vs_(\d+)', dataset)
            if match:
                digit_a, digit_b = int(match.group(1)), int(match.group(2))
                set_mnist_options('raw', (digit_a, digit_b))
                dataset_to_load = 'mnist'
            else:
                dataset_to_load = dataset
        else:
            dataset_to_load = dataset
        
        return get_shared_pipeline(dataset_to_load)
    except Exception as e:
        print(f"âŒ Erro ao carregar pipeline: {e}")
        return None


def gerar_perturbacoes(
    instancia_original: np.ndarray,
    features_fixas: List[int],
    X_train: pd.DataFrame,
    n_perturbacoes: int = 1000,
    estrategia: str = "uniform",
    pipeline = None,
    y_pred: int = None,
    scaler = None
) -> np.ndarray:
    """
    Gera perturbaÃ§Ãµes de uma instÃ¢ncia fixando features da explicaÃ§Ã£o.
    
    Args:
        instancia_original: InstÃ¢ncia original (vetor de features)
        features_fixas: Ãndices das features da explicaÃ§Ã£o (fixar valores)
        X_train: Dados de treino (para distribuiÃ§Ã£o)
        n_perturbacoes: NÃºmero de perturbaÃ§Ãµes a gerar
        estrategia: 'uniform', 'distribution', ou 'adversarial_worst_case'
        pipeline: Pipeline do modelo (para estratÃ©gia adversarial_worst_case)
        y_pred: PrediÃ§Ã£o original (para estratÃ©gia adversarial_worst_case)
        scaler: Scaler do pipeline (para estratÃ©gia adversarial_worst_case)
    
    Returns:
        Array (n_perturbacoes, n_features) com perturbaÃ§Ãµes
    """
    n_features = len(instancia_original)
    perturbacoes = np.tile(instancia_original, (n_perturbacoes, 1))
    
    # Features que serÃ£o perturbadas (nÃ£o estÃ£o na explicaÃ§Ã£o)
    features_perturbar = [i for i in range(n_features) if i not in features_fixas]
    
    if len(features_perturbar) == 0:
        # ExplicaÃ§Ã£o usa todas as features, nada a perturbar
        return perturbacoes
    
    # Obter valores min/max do dataset
    X_train_arr = X_train.values if hasattr(X_train, 'values') else X_train
    
    for feat_idx in features_perturbar:
        feat_min = X_train_arr[:, feat_idx].min()
        feat_max = X_train_arr[:, feat_idx].max()
        
        if estrategia == "uniform":
            # Valores aleatÃ³rios uniformes [min, max]
            perturbacoes[:, feat_idx] = np.random.uniform(feat_min, feat_max, n_perturbacoes)
        
        elif estrategia == "distribution":
            # Sample da distribuiÃ§Ã£o real do treino
            perturbacoes[:, feat_idx] = np.random.choice(
                X_train_arr[:, feat_idx], 
                size=n_perturbacoes, 
                replace=True
            )
        
        elif estrategia == "adversarial":
            # Valores extremos (50% min, 50% max)
            n_min = n_perturbacoes // 2
            perturbacoes[:n_min, feat_idx] = feat_min
            perturbacoes[n_min:, feat_idx] = feat_max
        
        elif estrategia == "adversarial_worst_case" and pipeline is not None and y_pred is not None:
            # Worst-case: escolhe min ou max baseado no coeficiente para empurrar score
            # na direÃ§Ã£o CONTRÃRIA Ã  prediÃ§Ã£o (mais adversarial)
            try:
                # Obter coeficientes do modelo
                if hasattr(pipeline, 'named_steps'):
                    logreg = pipeline.named_steps.get('classifier')
                    if logreg is None:
                        logreg = pipeline.named_steps.get('logisticregression')
                else:
                    logreg = pipeline
                
                if logreg is not None and hasattr(logreg, 'coef_'):
                    coef = logreg.coef_[0][feat_idx]
                    
                    # Para positivas (y_pred=1): empurrar para BAIXO (usar valores que diminuem score)
                    # Para negativas (y_pred=0): empurrar para CIMA (usar valores que aumentam score)
                    if y_pred == 1:
                        # AdversÃ¡rio quer DIMINUIR score: coef > 0 â†’ min, coef < 0 â†’ max
                        valor_adversarial = feat_min if coef > 0 else feat_max
                    else:
                        # AdversÃ¡rio quer AUMENTAR score: coef > 0 â†’ max, coef < 0 â†’ min
                        valor_adversarial = feat_max if coef > 0 else feat_min
                    
                    perturbacoes[:, feat_idx] = valor_adversarial
                else:
                    # Fallback para uniform se nÃ£o conseguir coeficientes
                    perturbacoes[:, feat_idx] = np.random.uniform(feat_min, feat_max, n_perturbacoes)
            except Exception:
                # Fallback para uniform em caso de erro
                perturbacoes[:, feat_idx] = np.random.uniform(feat_min, feat_max, n_perturbacoes)
        else:
            # Fallback para uniform
            perturbacoes[:, feat_idx] = np.random.uniform(feat_min, feat_max, n_perturbacoes)
    
    return perturbacoes


def calcular_baseline_predicao(
    pipeline,
    X_train: pd.DataFrame,
    y_pred: int,
    rejeitada: bool,
    t_plus: float,
    t_minus: float,
    max_abs: float = None,
    n_samples: int = 500
) -> float:
    """
    Calcula o baseline: probabilidade de manter a prediÃ§Ã£o por acaso
    quando TODAS as features sÃ£o perturbadas uniformemente.
    
    NOTA: Este valor Ã© calculado apenas para REPORTAR no artigo.
    NÃƒO Ã© usado para ajustar o threshold de 95% (que Ã© fixo).
    
    O baseline ajuda a INTERPRETAR os resultados de minimalidade:
    - Se baseline Ã© alto (ex: 98%) e minimalidade Ã© alta â†’ esperado (fÃ¡cil manter)
    - Se baseline Ã© baixo (ex: 2%) e minimalidade Ã© baixa â†’ esperado (difÃ­cil manter)
    
    Returns:
        float: Taxa base esperada de manutenÃ§Ã£o da prediÃ§Ã£o (0-1)
    """
    X_train_arr = X_train.values if hasattr(X_train, 'values') else X_train
    n_features = X_train_arr.shape[1]
    
    # Gerar perturbaÃ§Ãµes 100% uniformes (sem fixar nada)
    perturbacoes = np.zeros((n_samples, n_features))
    for feat_idx in range(n_features):
        feat_min = X_train_arr[:, feat_idx].min()
        feat_max = X_train_arr[:, feat_idx].max()
        perturbacoes[:, feat_idx] = np.random.uniform(feat_min, feat_max, n_samples)
    
    try:
        predicoes = pipeline.predict(perturbacoes)
        scores = pipeline.decision_function(perturbacoes)
        
        if max_abs is not None and max_abs > 0:
            scores = scores / max_abs
        
        if rejeitada:
            acertos = np.sum((scores >= t_minus) & (scores <= t_plus))
        else:
            acertos = np.sum(predicoes == y_pred)
        
        return acertos / n_samples
    except Exception:
        return 0.5  # Fallback: assume 50%


def validar_necessidade_features(
    instancia_idx: int,
    explicacao_features: List[str],
    feature_names: List[str],
    y_pred: int,
    rejeitada: bool,
    pipeline,
    X_test: pd.DataFrame,
    X_train: pd.DataFrame,
    t_plus: float,
    t_minus: float,
    n_perturbacoes: int = 200,
    max_abs: float = None,
    baseline_cache: Dict = None,
    modo: str = "local"
) -> Dict:
    """
    Testa se cada feature na explicaÃ§Ã£o Ã© NECESSÃRIA.
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    DOIS MODOS DE VALIDAÃ‡ÃƒO (2025-12-18):
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    ğŸ”¹ modo="local" â†’ ROBUSTEZ LOCAL (para PEAB e mÃ©todos heurÃ­sticos)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Conceito: Necessidade LOCAL = robustez empÃ­rica no entorno da instÃ¢ncia.
    
    Metodologia (baseada em PI/AXp - NeurIPS 2020):
      1. Define epsilon-ball: X_i âˆˆ [X_i_original - Îµ, X_i_original + Îµ]
      2. Remove feature testada (zera coeficiente)
      3. Perturba features nÃ£o-explicativas no epsilon-ball
      4. Busca contraexemplo: âˆƒ x_local que mantÃ©m decisÃ£o?
    
    InterpretaÃ§Ã£o:
      - Feature Ã© NECESSÃRIA: se NÃƒO existe perturbaÃ§Ã£o local que mantÃ©m decisÃ£o
      - Feature Ã© REDUNDANTE: se EXISTE perturbaÃ§Ã£o local que mantÃ©m decisÃ£o
      - Mede robustez empÃ­rica, nÃ£o suficiÃªncia lÃ³gica global
    
    ParÃ¢metros adaptativos:
      - EPSILON_FRACTION: 3-12% (escala com tamanho do dataset)
      - DELTA: 2-4% (margem numÃ©rica para evitar flips)
      - N_SAMPLES: 200 (busca amostragem no entorno)
    
    Resultados esperados:
      - Positivas/Negativas: necessidade â‰ˆ 60-90%
      - Rejeitadas: necessidade â‰ˆ 40-80%
      - RedundÃ¢ncia: > 0% (detecta features desnecessÃ¡rias)
    
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    ğŸ”¹ modo="global" â†’ VIABILIDADE DE LP (para PuLP/AXp e mÃ©todos Ã³timos)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Conceito: Necessidade GLOBAL = viabilidade lÃ³gica via LP solver.
    
    DefiniÃ§Ã£o matemÃ¡tica rigorosa:
      Feature f Ã© NECESSÃRIA âŸº LP sem f Ã© INFEASIBLE
      Feature f Ã© REDUNDANTE âŸº LP sem f Ã© FEASIBLE
    
    Metodologia (baseada em AXp/Abductive Explanations + LP):
      1. Remove feature testada (zera coeficiente w_i)
      2. Monta problema de viabilidade LP:
         VariÃ¡veis: x_j âˆˆ [min_j, max_j] para j â‰  i
         RestriÃ§Ã£o: wÂ·x + b {â‰¥, â‰¤, âˆˆ} threshold (depende do tipo)
      3. Resolve LP com PuLP CBC solver
      4. Verifica status:
         - INFEASIBLE â†’ feature Ã© NECESSÃRIA
         - FEASIBLE/OPTIMAL â†’ feature Ã© REDUNDANTE
    
    InterpretaÃ§Ã£o:
      - Feature Ã© NECESSÃRIA: impossÃ­vel satisfazer inequaÃ§Ã£o sem ela
      - Feature Ã© REDUNDANTE: existe vetor x que mantÃ©m decisÃ£o sem ela
      - Teste determinÃ­stico rigoroso (nÃ£o probabilÃ­stico)
    
    ImplementaÃ§Ã£o:
      - SEM epsilon-ball (usa bounds globais [min_dataset, max_dataset])
      - SEM amostragem (solver determinÃ­stico)
      - SEM critÃ©rios probabilÃ­sticos (np.any, etc)
      - USA programaÃ§Ã£o linear para teste de viabilidade
    
    Resultados esperados:
      - ExplicaÃ§Ãµes Ã³timas (PuLP): necessidade â‰ˆ 60-100% (depende do dataset)
      - Detecta redundÃ¢ncia matemÃ¡tica rigorosa
      - Mais rigoroso que validaÃ§Ã£o local (amostragem)
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    DIFERENÃ‡A CONCEITUAL:
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    LOCAL (PEAB):
      - "Feature resiste a perturbaÃ§Ãµes no entorno?" (robustez empÃ­rica)
      - Usa amostragem + np.any para buscar contraexemplo
      - LÃ“GICA PRESERVADA - NÃƒO MODIFICADA
    
    GLOBAL (PuLP):
      - "Feature Ã© logicamente necessÃ¡ria?" (viabilidade matemÃ¡tica)
      - Usa LP solver + verificaÃ§Ã£o de INFEASIBILITY
      - LÃ“GICA CORRIGIDA - substituÃ­da amostragem por LP
    
    AplicaÃ§Ã£o:
      - PEAB (heurÃ­stico) â†’ modo="local" (nÃ£o garante otimalidade global)
      - PuLP (Ã³timo) â†’ modo="global" (necessidade por construÃ§Ã£o matemÃ¡tica)
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    IMPORTANTE:
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    A lÃ³gica do modo LOCAL (PEAB) foi mantida EXATAMENTE como estava.
    Apenas o modo GLOBAL (PuLP) foi modificado para usar LP solver em vez de
    amostragem, corrigindo o problema conceitual de usar np.any para validar
    mÃ©todos de otimizaÃ§Ã£o.
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Args:
        modo: "local" (PEAB) ou "global" (PuLP/AXp)
    
    Returns:
        Dict com: necessary_count, redundant_features, necessity_score, baseline
    """
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CONFIGURAÃ‡ÃƒO: NormalizaÃ§Ã£o e parÃ¢metros base
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    n_features = len(feature_names)
    n_explicacao = len(explicacao_features)
    
    # Normalizar thresholds se necessÃ¡rio
    if max_abs is not None and max_abs > 0:
        t_plus_norm = t_plus
        t_minus_norm = t_minus
    else:
        t_plus_norm = t_plus  
        t_minus_norm = t_minus
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ROTEAMENTO: Delegar para modo LOCAL ou GLOBAL
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    if modo == "global":
        return _validar_necessidade_global(
            instancia_idx, explicacao_features, feature_names,
            y_pred, rejeitada, pipeline, X_test, X_train,
            t_plus_norm, t_minus_norm, max_abs, baseline_cache
        )
    else:  # modo == "local" (padrÃ£o)
        return _validar_necessidade_local(
            instancia_idx, explicacao_features, feature_names,
            y_pred, rejeitada, pipeline, X_test, X_train,
            t_plus_norm, t_minus_norm, n_perturbacoes, max_abs, baseline_cache
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODO LOCAL: Necessidade via robustez no epsilon-ball (PEAB)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _validar_necessidade_local(
    instancia_idx: int,
    explicacao_features: List[str],
    feature_names: List[str],
    y_pred: int,
    rejeitada: bool,
    pipeline,
    X_test: pd.DataFrame,
    X_train: pd.DataFrame,
    t_plus_norm: float,
    t_minus_norm: float,
    n_perturbacoes: int,
    max_abs: float,
    baseline_cache: Dict
) -> Dict:
    """
    VALIDAÃ‡ÃƒO LOCAL: Busca contraexemplo no epsilon-ball.
    
    Conceito:
        Feature Ã© NECESSÃRIA se nÃ£o existe perturbaÃ§Ã£o LOCAL que mantÃ©m decisÃ£o.
        Testa robustez empÃ­rica no entorno da instÃ¢ncia.
    
    Metodologia:
        1. Define epsilon-ball ao redor da instÃ¢ncia
        2. Remove feature testada (zera coeficiente)
        3. Perturba features nÃ£o-explicativas no epsilon-ball
        4. Se alguma configuraÃ§Ã£o mantÃ©m decisÃ£o â†’ REDUNDANTE
    """
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ParÃ¢metros adaptativos para epsilon-ball
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    n_features = len(feature_names)
    n_explicacao = len(explicacao_features)
    
    # Epsilon adaptativo: escala inversamente com nÃºmero de features
    if n_features <= 10:
        base_epsilon = 0.12
    elif n_features <= 50:
        base_epsilon = 0.10
    elif n_features <= 100:
        base_epsilon = 0.06
    else:
        base_epsilon = 0.03  # Datasets grandes (MNIST, etc)
    
    # Ajustar baseado no tamanho da explicaÃ§Ã£o
    explicacao_ratio = n_explicacao / n_features
    if explicacao_ratio < 0.10:
        epsilon_adj = 0.7  # ExplicaÃ§Ãµes pequenas â†’ epsilon menor
    elif explicacao_ratio < 0.30:
        epsilon_adj = 0.85
    else:
        epsilon_adj = 1.0
    
    EPSILON_FRACTION = base_epsilon * epsilon_adj
    
    # Delta adaptativo: margem numÃ©rica para evitar flips
    zona_rejeicao = abs(t_plus_norm - t_minus_norm)
    if zona_rejeicao > 0.5:
        DELTA = 0.04
    elif zona_rejeicao > 0.2:
        DELTA = 0.03
    else:
        DELTA = 0.02
    
    N_SAMPLES = n_perturbacoes
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    # Obter instÃ¢ncia original
    try:
        instancia_original = X_test.loc[instancia_idx].values
    except (KeyError, TypeError):
        try:
            instancia_original = X_test.iloc[int(instancia_idx)].values
        except (IndexError, ValueError):
            return {'necessary_count': len(explicacao_features), 'redundant_features': [], 'necessity_score': 100.0, 'baseline': 0.5}
    
    if len(explicacao_features) <= 1:
        return {'necessary_count': 1, 'redundant_features': [], 'necessity_score': 100.0, 'baseline': 0.5}
    
    # Extrair componentes do modelo
    if hasattr(pipeline, 'named_steps'):
        scaler = pipeline.named_steps.get('scaler')
        if 'model' in pipeline.named_steps:
            logreg = pipeline.named_steps['model']
        elif 'classifier' in pipeline.named_steps:
            logreg = pipeline.named_steps['classifier']
        else:
            logreg = pipeline.named_steps['logisticregression']
    else:
        return {'necessary_count': len(explicacao_features), 'redundant_features': [], 'necessity_score': 100.0, 'baseline': 0.5}
    
    coefs = logreg.coef_[0]
    intercept = logreg.intercept_[0]
    vals_s = scaler.transform(instancia_original.reshape(1, -1))[0]
    
    # Calcular min/max escalados do treino
    X_train_scaled = scaler.transform(X_train)
    min_scaled = X_train_scaled.min(axis=0)
    max_scaled = X_train_scaled.max(axis=0)
    
    # Definir epsilon-ball ao redor da instÃ¢ncia
    epsilon = EPSILON_FRACTION * (max_scaled - min_scaled)
    local_min = np.maximum(vals_s - epsilon, min_scaled)
    local_max = np.minimum(vals_s + epsilon, max_scaled)
    
    # Calcular baseline (apenas para reportar)
    cache_key = f"{y_pred}_{rejeitada}_local"
    if baseline_cache is not None and cache_key in baseline_cache:
        baseline = baseline_cache[cache_key]
    else:
        baseline = calcular_baseline_predicao(
            pipeline, X_train, y_pred, rejeitada, t_plus_norm, t_minus_norm, max_abs
        )
        if baseline_cache is not None:
            baseline_cache[cache_key] = baseline
    
    # Mapear nomes para Ã­ndices
    feature_to_idx = {name: idx for idx, name in enumerate(feature_names)}
    explicacao_idx = [feature_to_idx[f] for f in explicacao_features if f in feature_to_idx]
    
    features_redundantes = []
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LOOP: Testar cada feature da explicaÃ§Ã£o
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    for feat_name in explicacao_features:
        feat_idx = feature_to_idx.get(feat_name)
        if feat_idx is None:
            continue
        
        # Gerar N_SAMPLES perturbaÃ§Ãµes no entorno local
        samples = np.tile(vals_s, (N_SAMPLES, 1))
        
        # Identificar features NÃƒO EXPLICATIVAS (serÃ£o perturbadas no epsilon-ball)
        features_nao_explicativas = [i for i in range(len(feature_names)) 
                                      if i not in explicacao_idx]
        
        # Perturbar APENAS features NÃƒO EXPLICATIVAS no epsilon-ball
        for perturb_idx in features_nao_explicativas:
            samples[:, perturb_idx] = np.random.uniform(
                local_min[perturb_idx], 
                local_max[perturb_idx], 
                N_SAMPLES
            )
        
        # REMOVER feature testada: zerar seu coeficiente
        coefs_sem_feat = coefs.copy()
        coefs_sem_feat[feat_idx] = 0.0
        
        # Calcular scores SEM a feature testada
        scores = intercept + samples @ coefs_sem_feat
        
        if max_abs is not None and max_abs > 0:
            scores = scores / max_abs
        
        # Verificar se EXISTE contraexemplo (decisÃ£o mantida sem a feature)
        if rejeitada:
            scores_mantidos = (scores >= t_minus_norm + DELTA) & (scores <= t_plus_norm - DELTA)
            contraexemplo_existe = np.any(scores_mantidos)
        elif y_pred == 1:
            scores_mantidos = scores >= t_plus_norm - DELTA
            contraexemplo_existe = np.any(scores_mantidos)
        else:
            scores_mantidos = scores <= t_minus_norm + DELTA
            contraexemplo_existe = np.any(scores_mantidos)
        
        if contraexemplo_existe:
            features_redundantes.append(feat_name)
    
    necessary_count = len(explicacao_features) - len(features_redundantes)
    necessity_score = (necessary_count / len(explicacao_features)) * 100.0
    
    return {
        'necessary_count': necessary_count,
        'redundant_features': features_redundantes,
        'necessity_score': float(necessity_score),
        'baseline': float(baseline)
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODO GLOBAL: Necessidade via VIABILIDADE DE LP (PuLP/AXp)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _validar_necessidade_global(
    instancia_idx: int,
    explicacao_features: List[str],
    feature_names: List[str],
    y_pred: int,
    rejeitada: bool,
    pipeline,
    X_test: pd.DataFrame,
    X_train: pd.DataFrame,
    t_plus_norm: float,
    t_minus_norm: float,
    max_abs: float,
    baseline_cache: Dict
) -> Dict:
    """
    VALIDAÃ‡ÃƒO GLOBAL (PuLP): Testa necessidade via VIABILIDADE DE LP.
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    CONCEITO FUNDAMENTAL (diferente de PEAB):
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Feature f Ã© NECESSÃRIA âŸº LP sem f Ã© INFEASIBLE
    Feature f Ã© REDUNDANTE âŸº LP sem f Ã© FEASIBLE
    
    NÃƒO usa:
      âŒ Amostragem (np.any)
      âŒ PerturbaÃ§Ãµes
      âŒ CritÃ©rios probabilÃ­sticos
    
    USA:
      âœ… ProgramaÃ§Ã£o Linear (PuLP solver)
      âœ… VerificaÃ§Ã£o de INFEASIBILITY
      âœ… Teste determinÃ­stico rigoroso
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    METODOLOGIA (CORRIGIDA - VERSÃƒO FINAL):
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Para cada feature testada f_i na explicaÃ§Ã£o E:
    
      1. REMOVE feature testada (zera contribuiÃ§Ã£o w_i)
      2. FIXA TODAS as outras features nos valores ORIGINAIS da instÃ¢ncia
      3. Calcula score determinÃ­stico:
         score = intercept + Î£(w_j * valor_original_j) para j â‰  i
      4. Verifica se decisÃ£o Ã© mantida:
         - Positivas: score â‰¥ t+?
         - Negativas: score â‰¤ t-?
         - Rejeitadas: t- â‰¤ score â‰¤ t+?
      5. DecisÃ£o:
         - DecisÃ£o mantida â†’ feature Ã© REDUNDANTE
         - DecisÃ£o mudou â†’ feature Ã© NECESSÃRIA
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    POR QUE NÃƒO USA LP SOLVER?
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    VersÃ£o anterior (ERRADA):
      - Permitia features nÃ£o-explicativas variarem
      - Testava: "Existe configuraÃ§Ã£o global que compensa?"
      - Resultado: ~0-30% necessidade (features compensavam umas Ã s outras)
    
    VersÃ£o atual (CORRETA):
      - TODAS as features fixadas (exceto testada)
      - Testa: "E-{f_i} Ã© suficiente para a instÃ¢ncia ORIGINAL?"
      - Resultado: ~80-100% necessidade (PuLP gera explicaÃ§Ãµes minimais)
    
    NÃ£o precisa LP porque nÃ£o hÃ¡ variÃ¡veis livres. O score Ã© determinÃ­stico.
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    RESULTADOS ESPERADOS PARA PULP (MÃ‰TODO Ã“TIMO):
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    PuLP resolve ILP para encontrar explicaÃ§Ã£o MINIMAL. Portanto:
    
      - Necessidade esperada: 80-100% (a maioria das features sÃ£o necessÃ¡rias)
      - RedundÃ¢ncia esperada: 0-20% (muito baixa)
    
    Se necessidade < 50%:
      â†’ PossÃ­vel problema:
        1. PuLP nÃ£o estÃ¡ gerando explicaÃ§Ãµes minimais (bug no PuLP)
        2. Thresholds t+/t- muito permissivos
        3. InstÃ¢ncias na fronteira de decisÃ£o (mÃºltiplas explicaÃ§Ãµes vÃ¡lidas)
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    
    # Obter instÃ¢ncia original (USADA para fixar features explicativas)
    try:
        instancia_original = X_test.loc[instancia_idx].values
    except (KeyError, TypeError):
        try:
            instancia_original = X_test.iloc[int(instancia_idx)].values
        except (IndexError, ValueError):
            return {'necessary_count': len(explicacao_features), 'redundant_features': [], 'necessity_score': 100.0, 'baseline': 0.5}
    
    if len(explicacao_features) <= 1:
        return {'necessary_count': 1, 'redundant_features': [], 'necessity_score': 100.0, 'baseline': 0.5}
    
    # Extrair componentes do modelo
    if hasattr(pipeline, 'named_steps'):
        scaler = pipeline.named_steps.get('scaler')
        if 'model' in pipeline.named_steps:
            logreg = pipeline.named_steps['model']
        elif 'classifier' in pipeline.named_steps:
            logreg = pipeline.named_steps['classifier']
        else:
            logreg = pipeline.named_steps['logisticregression']
    else:
        return {'necessary_count': len(explicacao_features), 'redundant_features': [], 'necessity_score': 100.0, 'baseline': 0.5}
    
    coefs = logreg.coef_[0]
    intercept = logreg.intercept_[0]
    
    # Escalar instÃ¢ncia original (valores que serÃ£o fixados)
    instancia_scaled = scaler.transform(instancia_original.reshape(1, -1))[0]
    
    # Calcular bounds GLOBAIS do dataset (para features NÃƒO-EXPLICATIVAS)
    X_train_scaled = scaler.transform(X_train)
    min_scaled = X_train_scaled.min(axis=0)
    max_scaled = X_train_scaled.max(axis=0)
    
    # Calcular baseline (apenas para reportar)
    cache_key = f"{y_pred}_{rejeitada}_global"
    if baseline_cache is not None and cache_key in baseline_cache:
        baseline = baseline_cache[cache_key]
    else:
        baseline = calcular_baseline_predicao(
            pipeline, X_train, y_pred, rejeitada, t_plus_norm, t_minus_norm, max_abs
        )
        if baseline_cache is not None:
            baseline_cache[cache_key] = baseline
    
    # Mapear nomes para Ã­ndices
    feature_to_idx = {name: idx for idx, name in enumerate(feature_names)}
    explicacao_idx = [feature_to_idx[f] for f in explicacao_features if f in feature_to_idx]
    
    features_redundantes = []
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LOOP: Testar cada feature usando LP SOLVER
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    for feat_name in explicacao_features:
        feat_idx = feature_to_idx.get(feat_name)
        if feat_idx is None:
            continue
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # LÃ“GICA CORRETA PARA VALIDAR MÃ‰TODO Ã“TIMO (PuLP):
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 
        # Pergunta: "Remover feature f_i da explicaÃ§Ã£o E torna E-{f_i} 
        #            INSUFICIENTE para a instÃ¢ncia ORIGINAL?"
        # 
        # MÃ©todo:
        # - Feature TESTADA: REMOVIDA (zera w_i)
        # - TODAS as outras features: FIXADAS nos valores originais
        # 
        # NÃƒO permite variaÃ§Ã£o de features nÃ£o-explicativas!
        # Estamos testando se E-{f_i} Ã© suficiente para a instÃ¢ncia original,
        # nÃ£o se existe alguma configuraÃ§Ã£o global que funciona.
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        # SIMPLIFICAÃ‡ÃƒO: Calcular score diretamente (sem LP)
        # 
        # Como TODAS as features (exceto testada) estÃ£o FIXADAS,
        # nÃ£o hÃ¡ variÃ¡veis livres! O score Ã© determinÃ­stico.
        # 
        # score = intercept + Î£(w_j * valor_original_j) para j â‰  feat_idx
        
        score_sem_feat = intercept
        
        # Somar contribuiÃ§Ã£o de TODAS as features EXCETO a testada
        for j in range(len(feature_names)):
            if j != feat_idx:
                score_sem_feat += coefs[j] * instancia_scaled[j]
        
        # Normalizar se necessÃ¡rio
        if max_abs is not None and max_abs > 0:
            score_sem_feat = score_sem_feat / max_abs
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # VERIFICAR SE DECISÃƒO Ã‰ MANTIDA (sem LP, Ã© cÃ¡lculo direto)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        decisao_mantida = False
        
        if rejeitada:
            # Rejeitada: score deve estar na zona [t-, t+]
            decisao_mantida = (score_sem_feat >= t_minus_norm) and (score_sem_feat <= t_plus_norm)
        elif y_pred == 1:
            # Positiva: score >= t+
            decisao_mantida = (score_sem_feat >= t_plus_norm)
        else:  # y_pred == 0
            # Negativa: score <= t-
            decisao_mantida = (score_sem_feat <= t_minus_norm)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # DECISÃƒO:
        # Se decisÃ£o mantida â†’ feature Ã© REDUNDANTE
        # Se decisÃ£o mudou â†’ feature Ã© NECESSÃRIA
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        if decisao_mantida:
            features_redundantes.append(feat_name)
    
    necessary_count = len(explicacao_features) - len(features_redundantes)
    necessity_score = (necessary_count / len(explicacao_features)) * 100.0
    
    return {
        'necessary_count': necessary_count,
        'redundant_features': features_redundantes,
        'necessity_score': float(necessity_score),
        'baseline': float(baseline)
    }


def validar_fidelity_instancia(
    instancia_idx: int,
    explicacao_features: List[str],
    feature_names: List[str],
    y_true: int,
    y_pred: int,
    rejeitada: bool,
    pipeline,
    X_test: pd.DataFrame,
    X_train: pd.DataFrame,
    t_plus: float,
    t_minus: float,
    n_perturbacoes: int = 1000,
    estrategia: str = "uniform",
    max_abs: float = None
) -> Dict:
    """
    Valida fidelity de uma instÃ¢ncia usando perturbaÃ§Ãµes.
    
    Returns:
        Dict com mÃ©tricas: fidelity, sufficiency, perturbations_tested, etc.
    """
    # Obter instÃ¢ncia original
    # Tentar primeiro usar label-based indexing (.loc), depois position-based (.iloc)
    try:
        # Tentar como label do Ã­ndice (que Ã© o que PEAB salva no JSON)
        instancia_original = X_test.loc[instancia_idx].values
    except (KeyError, TypeError):
        try:
            # Se falhar, tentar como Ã­ndice posicional (posiÃ§Ã£o)
            instancia_original = X_test.iloc[int(instancia_idx)].values
        except (IndexError, ValueError):
            # Se ainda falhar, logar erro e retornar None
            return {
                'fidelity': -1,
                'sufficiency': -1,
                'explanation_size': len(explicacao_features),
                'perturbations_tested': 0,
                'error': f"NÃ£o foi possÃ­vel acessar instÃ¢ncia {instancia_idx} em X_test"
            }
    
    # Mapear nomes de features para Ã­ndices
    features_fixas_idx = [feature_names.index(feat) for feat in explicacao_features if feat in feature_names]
    
    # Gerar perturbaÃ§Ãµes
    perturbacoes = gerar_perturbacoes(
        instancia_original,
        features_fixas_idx,
        X_train,
        n_perturbacoes,
        estrategia
    )
    
    # Reclassificar perturbaÃ§Ãµes
    try:
        predicoes = pipeline.predict(perturbacoes)
        scores = pipeline.decision_function(perturbacoes)
        
        # [BUGFIX] Normalizar scores se max_abs foi fornecido (PEAB/PuLP)
        # PEAB normaliza com: score_norm = score_raw / max_abs
        # Os thresholds t_plus e t_minus jÃ¡ estÃ£o em espaÃ§o normalizado
        if max_abs is not None and max_abs > 0:
            scores = scores / max_abs
        
    except Exception as e:
        print(f"âš ï¸  Erro ao reclassificar instÃ¢ncia {instancia_idx}: {e}")
        return {
            'fidelity': 0.0,
            'sufficiency': 0.0,
            'perturbations_tested': 0,
            'perturbations_correct': 0,
            'error': str(e)
        }
    
    # Contar acertos baseado no tipo de prediÃ§Ã£o original
    if rejeitada:
        # InstÃ¢ncia rejeitada: todas as perturbaÃ§Ãµes devem cair na zona de rejeiÃ§Ã£o
        # Se a explicaÃ§Ã£o estÃ¡ correta, fixar as features essenciais deve manter a rejeiÃ§Ã£o
        acertos = np.sum((scores >= t_minus) & (scores <= t_plus))
    else:
        # InstÃ¢ncia aceita: perturbaÃ§Ãµes devem ter mesma classe
        acertos = np.sum(predicoes == y_pred)
    
    fidelity = (acertos / n_perturbacoes) * 100.0
    
    return {
        'fidelity': float(fidelity),
        'sufficiency': float(fidelity),  # Para mÃ©todos Ã³timos, suficiÃªncia = fidelity
        'perturbations_tested': int(n_perturbacoes),
        'perturbations_correct': int(acertos)
    }


def validar_metodo(
    metodo: str,
    dataset: str,
    n_perturbacoes: int = None,
    estrategia: str = None,
    verbose: bool = True,
    modo_necessidade: str = None
) -> Dict:
    """
    Valida um mÃ©todo completo (PEAB, PuLP, Anchor, MinExp).
    
    Args:
        metodo: Nome do mÃ©todo
        dataset: Nome do dataset
        n_perturbacoes: NÃºmero de perturbaÃ§Ãµes (None = usar padrÃ£o automÃ¡tico)
        estrategia: EstratÃ©gia de perturbaÃ§Ã£o (None = usar PERTURBATION_STRATEGY)
        verbose: Mostrar progresso
        modo_necessidade: "local" (PEAB) ou "global" (PuLP/AXp). None = auto-detect
    
    Returns:
        DicionÃ¡rio com todas as mÃ©tricas de validaÃ§Ã£o
    """
    # Carregar resultados do mÃ©todo (retorna tupla com dataset correto)
    resultado_carga = carregar_resultados_metodo(metodo, dataset)
    if resultado_carga is None:
        return None
    
    resultados, dataset_correto = resultado_carga
    
    # [BUGFIX] Extrair max_abs do JSON se disponÃ­vel (PEAB/PuLP usam normalizaÃ§Ã£o)
    max_abs = None
    if 'thresholds' in resultados and 'normalization' in resultados['thresholds']:
        max_abs = resultados['thresholds']['normalization'].get('max_abs', None)
    
    if verbose:
        print(f"\n{'â•'*70}")
        print(f"  VALIDANDO: {metodo.upper()} - Dataset: {dataset_correto}")
        print(f"{'â•'*70}")
    
    # Carregar pipeline e dados (dataset_correto jÃ¡ contÃ©m MNIST_X_vs_Y se necessÃ¡rio)
    pipeline_data = carregar_pipeline_dataset(dataset_correto)
    if pipeline_data is None:
        return None
    
    pipeline, X_train, X_test, y_train, y_test, t_plus, t_minus, meta = pipeline_data
    feature_names = meta['feature_names']
    
    # Determinar nÃºmero de perturbaÃ§Ãµes automaticamente se nÃ£o especificado
    if n_perturbacoes is None:
        num_features = len(feature_names)
        num_instances = len(X_test)
        
        # Ajuste automÃ¡tico: datasets grandes â†’ menos perturbaÃ§Ãµes
        if num_features >= 500 or num_instances > 1000:
            n_perturbacoes = NUM_PERTURBATIONS_LARGE
            if verbose:
                print(f"[AUTO] Dataset grande detectado: usando {n_perturbacoes} perturbaÃ§Ãµes")
        else:
            n_perturbacoes = NUM_PERTURBATIONS_DEFAULT
    
    # Usar estratÃ©gia padrÃ£o se nÃ£o especificada
    if estrategia is None:
        estrategia = PERTURBATION_STRATEGY
    
    # Auto-detectar modo de necessidade baseado no mÃ©todo
    if modo_necessidade is None:
        if metodo.upper() in ['PULP', 'AXP']:
            modo_necessidade = "global"
            if verbose:
                print(f"[AUTO] MÃ©todo Ã³timo detectado: usando validaÃ§Ã£o GLOBAL (viabilidade lÃ³gica)")
        else:
            modo_necessidade = "local"
            if verbose:
                print(f"[AUTO] MÃ©todo heurÃ­stico detectado: usando validaÃ§Ã£o LOCAL (epsilon-ball)")
    
    # Obter explicaÃ§Ãµes do JSON
    explicacoes = resultados.get('explicacoes', resultados.get('per_instance', []))
    
    if not explicacoes:
        print(f"\nâŒ ERRO: Nenhuma explicaÃ§Ã£o individual encontrada em {metodo}_results.json")
        print(f"\n{'â”€'*70}")
        print("SOLUÃ‡ÃƒO:")
        print(f"  1. O arquivo {metodo.lower()}_results.json existe mas NÃƒO contÃ©m explicaÃ§Ãµes individuais")
        print(f"  2. Execute novamente o mÃ©todo para gerar explicaÃ§Ãµes completas:")
        print(f"     python {metodo.lower()}.py")
        print(f"  3. Selecione o dataset: {dataset}")
        print(f"\n  NOTA: O JSON atual contÃ©m apenas estatÃ­sticas agregadas.")
        print(f"        A validaÃ§Ã£o precisa das explicaÃ§Ãµes individuais (por instÃ¢ncia).")
        print(f"{'â”€'*70}\n")
        return None
    
    if verbose:
        print(f"â†’ Validando {len(explicacoes)} explicaÃ§Ãµes...")
        print(f"â†’ PerturbaÃ§Ãµes por instÃ¢ncia: {n_perturbacoes}")
        print(f"â†’ EstratÃ©gia: {estrategia}")
    
    # Inicializar mÃ©tricas
    metricas_por_instancia = []
    tamanhos_explicacao = []
    fidelities = []
    
    # MÃ©tricas por tipo
    metricas_por_tipo = {
        'positive': {'fidelities': [], 'sizes': [], 'necessities': [], 'count': 0},
        'negative': {'fidelities': [], 'sizes': [], 'necessities': [], 'count': 0},
        'rejected': {'fidelities': [], 'sizes': [], 'necessities': [], 'count': 0}
    }
    
    # DistribuiÃ§Ã£o de tamanhos
    size_distribution = defaultdict(int)
    
    # Cache para baseline (evita recalcular para cada instÃ¢ncia)
    baseline_cache = {}
    
    # Tempo de inÃ­cio
    start_time = time.time()
    
    # Validar cada explicaÃ§Ã£o
    from utils.progress_bar import ProgressBar
    
    with ProgressBar(total=len(explicacoes), description=f"Validando {metodo}") as pbar:
        for exp in explicacoes:
            idx = exp.get('indice', exp.get('id'))
            if idx is None:
                pbar.update()
                continue
            
            idx = int(idx)
            
            # Extrair informaÃ§Ãµes da explicaÃ§Ã£o - suporta mÃºltiplos formatos
            # Formato PEAB: 'explanation' + 'explanation_size'
            # Formato PuLP: 'features_selecionadas' + 'tamanho'
            # Formato antigo: 'explicacao' ou 'features'
            if 'explanation' in exp:
                explicacao_features = exp['explanation']
                tamanho = exp.get('explanation_size', len(explicacao_features))
            elif 'features_selecionadas' in exp:
                explicacao_features = exp['features_selecionadas']
                tamanho = exp.get('tamanho', len(explicacao_features))
            elif 'explicacao' in exp:
                explicacao_features = exp['explicacao']
                tamanho = len(explicacao_features)
            elif 'features' in exp:
                explicacao_features = exp['features']
                tamanho = len(explicacao_features)
            else:
                pbar.update()
                continue
            
            tamanhos_explicacao.append(tamanho)
            
            # Contar distribuiÃ§Ã£o de tamanhos
            if tamanho >= 6:
                size_distribution['6+'] += 1
            else:
                size_distribution[str(tamanho)] += 1
            
            # Extrair y_true e y_pred com suporte a mÃºltiplos formatos
            # Formato PEAB: 'y_true', 'y_pred', 'rejected' (bool)
            # Formato PuLP: 'classe_real', 'tipo_predicao' (string)
            y_true = exp.get('y_true', -1)
            if y_true == -1 and 'classe_real' in exp:
                # PuLP usa string, converter para int
                y_true = 1 if 'DiabÃ©tico' in str(exp['classe_real']) else 0
            y_true = int(y_true)
            
            y_pred = int(exp.get('y_pred', exp.get('predicao', -1)))
            
            # Detectar se Ã© rejeitada
            # Formato PEAB: 'rejected' (bool)
            # Formato PuLP: 'tipo_predicao' == 'REJEITADA'
            rejeitada = bool(exp.get('rejected', exp.get('rejeitada', False)))
            if not rejeitada and 'tipo_predicao' in exp:
                rejeitada = ('REJEIT' in exp['tipo_predicao'].upper())
            
            # Determinar tipo
            if rejeitada:
                tipo = 'rejected'
            elif 'tipo_predicao' in exp:
                # PuLP usa string
                tipo_pred = exp['tipo_predicao'].upper()
                if 'POSIT' in tipo_pred:
                    tipo = 'positive'
                    y_pred = 1
                elif 'NEGAT' in tipo_pred:
                    tipo = 'negative'
                    y_pred = 0
                else:
                    tipo = 'rejected'
                    y_pred = -1
            elif y_pred == 1:
                tipo = 'positive'
            elif y_pred == 0:
                tipo = 'negative'
            else:
                tipo = 'rejected'
            
            # Validar fidelity
            resultado = validar_fidelity_instancia(
                idx,
                explicacao_features,
                feature_names,
                y_true,
                y_pred,
                rejeitada,
                pipeline,
                X_test,
                X_train,
                t_plus,
                t_minus,
                n_perturbacoes,
                estrategia,
                max_abs
            )
            
            fidelity = resultado['fidelity']
            
            # Se houver erro ao processar a instÃ¢ncia, pular
            if 'error' in resultado:
                pbar.update()
                continue
            
            # Validar necessidade (minimalidade) - apenas para explicaÃ§Ãµes com 2+ features
            resultado_necessidade = {'necessary_count': tamanho, 'redundant_features': [], 'necessity_score': 100.0, 'baseline': 0.5}
            if tamanho >= 2:
                resultado_necessidade = validar_necessidade_features(
                    idx,
                    explicacao_features,
                    feature_names,
                    y_pred,
                    rejeitada,
                    pipeline,
                    X_test,
                    X_train,
                    t_plus,
                    t_minus,
                    n_perturbacoes=200,  # Menos perturbaÃ§Ãµes para ser mais rÃ¡pido
                    max_abs=max_abs,
                    baseline_cache=baseline_cache,
                    modo=modo_necessidade  # [NOVO] Passa o modo (local/global)
                )
            
            fidelities.append(fidelity)
            
            # Atualizar mÃ©tricas por tipo
            metricas_por_tipo[tipo]['fidelities'].append(fidelity)
            metricas_por_tipo[tipo]['sizes'].append(tamanho)
            metricas_por_tipo[tipo]['necessities'].append(resultado_necessidade['necessity_score'])
            metricas_por_tipo[tipo]['count'] += 1
            
            # Armazenar resultado
            metricas_por_instancia.append({
                'instance_id': idx,
                'y_true': y_true,
                'y_pred': y_pred,
                'rejected': rejeitada,
                'explanation_size': tamanho,
                'explanation_features': explicacao_features,
                'fidelity': fidelity,
                'sufficiency': resultado['sufficiency'],
                'perturbations_tested': resultado['perturbations_tested'],
                'perturbations_correct': resultado['perturbations_correct'],
                'necessary_features': resultado_necessidade['necessary_count'],
                'redundant_features': resultado_necessidade['redundant_features'],
                'necessity_score': resultado_necessidade['necessity_score']
            })
            
            pbar.update()
    
    # Calcular tempo total
    validation_time = time.time() - start_time
    
    # Calcular mÃ©tricas globais
    fidelity_overall = np.mean(fidelities)
    
    # Calcular mÃ©tricas por tipo
    per_type_metrics = {}
    necessities_all = []
    for tipo, dados in metricas_por_tipo.items():
        if dados['count'] > 0:
            necessities_all.extend(dados['necessities'])
            per_type_metrics[tipo] = {
                'count': dados['count'],
                'fidelity': float(np.mean(dados['fidelities'])),
                'necessity': float(np.mean(dados['necessities'])),
                'mean_size': float(np.mean(dados['sizes'])),
                'std_size': float(np.std(dados['sizes']))
            }
        else:
            per_type_metrics[tipo] = {
                'count': 0,
                'fidelity': 0.0,
                'necessity': 0.0,
                'mean_size': 0.0,
                'std_size': 0.0
            }
    
    # Necessity geral
    necessity_overall = float(np.mean(necessities_all)) if necessities_all else 0.0
    
    # Calcular reduction rate
    num_features = len(feature_names)
    
    # [BUGFIX] Verificar se hÃ¡ explicaÃ§Ãµes antes de calcular estatÃ­sticas
    if len(tamanhos_explicacao) > 0:
        mean_size = np.mean(tamanhos_explicacao)
        median_size = np.median(tamanhos_explicacao)
        std_size = np.std(tamanhos_explicacao)
        min_size = int(np.min(tamanhos_explicacao))
        max_size = int(np.max(tamanhos_explicacao))
        reduction_rate = ((num_features - mean_size) / num_features) * 100.0
    else:
        mean_size = 0.0
        median_size = 0.0
        std_size = 0.0
        min_size = 0
        max_size = 0
        reduction_rate = 0.0
    
    # Montar resultado final
    resultado_validacao = {
        'metadata': {
            'method': metodo,
            'dataset': dataset,
            'date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'num_perturbations': n_perturbacoes,
            'perturbation_strategy': estrategia,
            'necessity_mode': modo_necessidade,  # [NOVO] Documenta modo usado
            'test_instances': len(explicacoes),
            'num_features': num_features
        },
        'global_metrics': {
            'fidelity_overall': float(fidelity_overall),
            'necessity_overall': float(necessity_overall),
            'fidelity_positive': float(per_type_metrics['positive']['fidelity']),
            'fidelity_negative': float(per_type_metrics['negative']['fidelity']),
            'fidelity_rejected': float(per_type_metrics['rejected']['fidelity']),
            'necessity_positive': float(per_type_metrics['positive']['necessity']),
            'necessity_negative': float(per_type_metrics['negative']['necessity']),
            'necessity_rejected': float(per_type_metrics['rejected']['necessity']),
            'sufficiency': float(fidelity_overall),  # Para mÃ©todos Ã³timos
            'coverage': 100.0,  # % instÃ¢ncias sem erro
            'mean_explanation_size': float(mean_size),
            'median_explanation_size': float(median_size),
            'std_explanation_size': float(std_size),
            'min_explanation_size': min_size,
            'max_explanation_size': max_size,
            'reduction_rate': float(reduction_rate),
            'validation_time_seconds': float(validation_time)
        },
        'per_type_metrics': per_type_metrics,
        'size_distribution': dict(size_distribution),
        'per_instance_results': metricas_por_instancia
    }
    
    if verbose:
        print(f"\nâœ“ ValidaÃ§Ã£o completa!")
        print(f"  - Fidelity Geral: {fidelity_overall:.2f}%")
        print(f"  - Minimalidade Geral: {necessity_overall:.2f}%")
        print(f"  - Tamanho MÃ©dio: {mean_size:.2f}")
        print(f"  - Taxa de ReduÃ§Ã£o: {reduction_rate:.2f}%")
        print(f"  - Tempo: {validation_time:.2f}s")
    
    return resultado_validacao


def salvar_json_validacao(resultado: Dict, metodo: str, dataset: str):
    """Salva resultado da validaÃ§Ã£o em JSON."""
    json_path = os.path.join(VALIDATION_JSON_DIR, f"{metodo.lower()}_validation_{dataset}.json")
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(resultado, f, indent=2, ensure_ascii=False)
    
    print(f"âœ“ JSON salvo: {json_path}")


def gerar_relatorio_txt(resultado: Dict, metodo: str, dataset: str):
    """Gera relatÃ³rio TXT simplificado e cientÃ­fico."""
    
    # [ORGANIZAÃ‡ÃƒO] Estrutura: results/validation/{dataset}/{metodo}/
    output_dir = os.path.join(VALIDATION_RESULTS_DIR, metodo.lower(), dataset,)
    os.makedirs(output_dir, exist_ok=True)
    
    # Nome do arquivo: {metodo}_validation_{dataset}.txt
    report_filename = f"{metodo.lower()}_validation_{dataset}.txt"
    report_path = os.path.join(output_dir, report_filename)
    
    meta = resultado['metadata']
    globais = resultado['global_metrics']
    por_tipo = resultado['per_type_metrics']
    dist_size = resultado['size_distribution']
    
    # Converter nome do dataset para display
    dataset_display = dataset.replace('_', ' ').title()
    metodo_display = metodo.upper()
    
    with open(report_path, 'w', encoding='utf-8') as f:
        # CabeÃ§alho
        f.write("â•”" + "â•" * 78 + "â•—\n")
        f.write("â•‘" + " " * 78 + "â•‘\n")
        f.write("â•‘" + f"VALIDAÃ‡ÃƒO DE EXPLICABILIDADE - {metodo_display}".center(78) + "â•‘\n")
        f.write("â•‘" + f"{dataset_display}".center(78) + "â•‘\n")
        f.write("â•‘" + " " * 78 + "â•‘\n")
        f.write("â•š" + "â•" * 78 + "â•\n\n")
        
        # =====================================================================
        # RESUMO EXECUTIVO (NOVO)
        # =====================================================================
        f.write("â”" * 80 + "\n")
        f.write("RESUMO EXECUTIVO\n")
        f.write("â”" * 80 + "\n\n")
        
        f.write(f"  Dataset:                {dataset_display}\n")
        f.write(f"  InstÃ¢ncias Testadas:    {meta['test_instances']}\n")
        f.write(f"  Features Totais:        {meta['num_features']}\n\n")
        
        f.write("  MÃ‰TRICAS PRINCIPAIS:\n")
        f.write(f"    â€¢ Fidelidade:                      {globais['fidelity_overall']:.1f}%\n")
        f.write(f"    â€¢ Necessidade (feat. necessÃ¡rias): {globais['necessity_overall']:.1f}%\n")
        f.write(f"    â€¢ Tamanho MÃ©dio:                   {globais['mean_explanation_size']:.1f} features\n")
        
        # Calcular taxa de rejeiÃ§Ã£o total
        rej_count = por_tipo['rejected']['count']
        taxa_rej = (rej_count / meta['test_instances']) * 100 if meta['test_instances'] > 0 else 0
        f.write(f"    â€¢ Taxa de RejeiÃ§Ã£o:     {taxa_rej:.1f}% ({rej_count} instÃ¢ncias)\n\n")
        
        # ConclusÃ£o curta baseada nas mÃ©tricas
        if globais['fidelity_overall'] >= 95 and globais['necessity_overall'] >= 80:
            conclusao = "ExplicaÃ§Ãµes de alta qualidade: fiÃ©is e minimais."
        elif globais['fidelity_overall'] >= 95:
            conclusao = "ExplicaÃ§Ãµes fiÃ©is, porÃ©m contÃªm features redundantes."
        elif globais['necessity_overall'] >= 80:
            conclusao = "ExplicaÃ§Ãµes minimais, mas fidelidade requer atenÃ§Ã£o."
        else:
            conclusao = "Qualidade variÃ¡vel: revisar mÃ©todo e hiperparÃ¢metros."
        
        f.write(f"  CONCLUSÃƒO:\n")
        f.write(f"    {conclusao}\n\n")
        f.write("â”" * 80 + "\n\n")
        
        # SEÃ‡ÃƒO 1: DescriÃ§Ã£o do MÃ©todo (Simplificada)
        f.write("â”" * 80 + "\n")
        f.write("METODOLOGIA DE VALIDAÃ‡ÃƒO\n")
        f.write("â”" * 80 + "\n\n")
        
        f.write(f"  MÃ©todo Avaliado:         {metodo_display}\n")
        f.write(f"  PerturbaÃ§Ãµes/instÃ¢ncia:  {meta['num_perturbations']:,}\n\n")
        
        f.write("  TESTES APLICADOS:\n\n")
        
        f.write("  1. FIDELIDADE (Sufficiency) - Teste ProbabilÃ­stico\n")
        f.write("     â€¢ Para cada feature da explicaÃ§Ã£o, geramos perturbaÃ§Ãµes e verificamos\n")
        f.write("       se o modelo mantÃ©m a decisÃ£o quando apenas essa feature estÃ¡ ativa.\n")
        f.write("     â€¢ CritÃ©rio: Feature Ã© fiel se >95% das perturbaÃ§Ãµes mantÃªm a decisÃ£o.\n")
        f.write("     â€¢ Objetivo: Garantir que features explicativas CAUSAM a decisÃ£o.\n\n")
        
        f.write("  2. NECESSIDADE (Minimality) - Teste DeterminÃ­stico (Worst-Case)\n")
        f.write("     â€¢ Para cada feature, construÃ­mos o cenÃ¡rio mais adverso possÃ­vel:\n")
        f.write("       removemos a feature e atribuÃ­mos valores extremos Ã s demais features\n")
        f.write("       nÃ£o-explicativas (pior caso que maximiza score positivo ou negativo).\n")
        f.write("     â€¢ CritÃ©rio: Feature Ã© necessÃ¡ria se sua remoÃ§Ã£o SEMPRE quebra a decisÃ£o\n")
        f.write("       no pior caso deterministicamente possÃ­vel.\n")
        f.write("     â€¢ Objetivo: Eliminar features redundantes (minimalidade).\n\n")
        
        f.write("  NOTA TÃ‰CNICA: Fidelidade Ã© suficiÃªncia estatÃ­stica (perturbaÃ§Ãµes),\n")
        f.write("                Necessidade Ã© teste lÃ³gico (existe caso adverso).\n\n")
        f.write("â”" * 80 + "\n\n")
        
        # SEÃ‡ÃƒO 2: ConfiguraÃ§Ã£o (Simplificada)
        f.write("â”" * 80 + "\n")
        f.write("CONFIGURAÃ‡ÃƒO\n")
        f.write("â”" * 80 + "\n\n")
        f.write(f"  Dataset:              {dataset_display}\n")
        f.write(f"  InstÃ¢ncias:           {meta['test_instances']}\n")
        f.write(f"  Features:             {meta['num_features']}\n")
        f.write(f"  PerturbaÃ§Ãµes/inst:    {meta['num_perturbations']:,}\n")
        f.write(f"  Data:                 {meta['date']}\n\n")
        f.write("â”" * 80 + "\n\n")
        
        # SEÃ‡ÃƒO 3: Resultados (Simplificado)
        f.write("â”" * 80 + "\n")
        f.write("RESULTADOS\n")
        f.write("â”" * 80 + "\n\n")
        
        redundancia_global = 100.0 - globais['necessity_overall']
        
        f.write("  MÃ‰TRICAS GLOBAIS:\n")
        f.write(f"    Fidelidade:       {globais['fidelity_overall']:.1f}%\n")
        f.write(f"    Necessidade:      {globais['necessity_overall']:.1f}%\n")
        f.write(f"    RedundÃ¢ncia:      {redundancia_global:.1f}%\n")
        f.write(f"    Cobertura:        {globais['coverage']:.1f}%\n")
        f.write(f"    Tamanho mÃ©dio:    {globais['mean_explanation_size']:.1f} features\n\n")
        
        f.write("  POR TIPO DE DECISÃƒO:\n\n")
        f.write("  â€œNecessidade Estrita (Worst-case)â€\n")
        f.write("  â€œO teste verifica se a feature Ã© necessÃ¡ria sob o pior cenÃ¡rio adversarial possÃ­vel, nÃ£o se ela Ã© Ãºnica explicaÃ§Ã£o possÃ­vel.â€\n\n")
        f.write("  Tipo          | Count |  Fidelidade | Necessidade | RedundÃ¢ncia\n")
        f.write("  " + "â”€" * 72 + "\n")
        for tipo_nome, tipo_label in [('positive', 'Positivas'), 
                                       ('negative', 'Negativas'), 
                                       ('rejected', 'Rejeitadas')]:
            dados = por_tipo[tipo_nome]
            redundancia_tipo = 100.0 - dados['necessity']
            f.write(f"  {tipo_label:12}  | {dados['count']:5} | {dados['fidelity']:10.1f}% | {dados['necessity']:10.1f}% | {redundancia_tipo:10.1f}%\n")
        f.write("\n")
        
        f.write("  TAMANHO DAS EXPLICAÃ‡Ã•ES:\n")
        f.write(f"    MÃ©dia:         {globais['mean_explanation_size']:.1f} features\n")
        f.write(f"    Mediana:       {globais['median_explanation_size']:.0f}\n")
        f.write(f"    Desvio:        {globais['std_explanation_size']:.1f}\n")
        f.write(f"    Intervalo:     [{globais['min_explanation_size']}, {globais['max_explanation_size']}]\n")
        f.write(f"    CompactaÃ§Ã£o:   {globais['reduction_rate']:.0f}% (vs {meta['num_features']} features totais)\n")
        f.write("\n")
        
        f.write("3.3 DISTRIBUIÃ‡ÃƒO DE TAMANHOS DAS EXPLICAÃ‡Ã•ES\n")
        f.write("â”€" * 80 + "\n\n")
        f.write("  VariÃ¡veis â”‚ Quantidade â”‚ Porcentagem â”‚ VisualizaÃ§Ã£o\n")
        f.write("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼" + "â”€" * 42 + "\n")
        
        total = meta['test_instances']
        for size in sorted(dist_size.keys(), key=lambda x: int(x.replace('+', '')) if x != '6+' else 6):
            count = dist_size[size]
            pct = (count / total) * 100
            bar_len = int(pct / 2)
            bar = "â–ˆ" * bar_len
            f.write(f"     {size:>4}    â”‚    {count:>4}    â”‚    {pct:>5.1f}%   â”‚ {bar:<40}\n")
        f.write("\n")
        
        # SEÃ‡ÃƒO 4: AnÃ¡lise por Tipo (Simplificada - jÃ¡ estÃ¡ na tabela acima)
        # Removida para evitar redundÃ¢ncia
        f.write("â”" * 80 + "\n\n")
        
        # SEÃ‡ÃƒO 5: InterpretaÃ§Ã£o CrÃ­tica
        f.write("â”" * 80 + "\n")
        f.write("INTERPRETAÃ‡ÃƒO DOS RESULTADOS\n")
        f.write("â”" * 80 + "\n\n")
        
        redundancia_pct = 100.0 - globais['necessity_overall']
        
        # AnÃ¡lise objetiva das mÃ©tricas
        f.write(f"  Fidelidade:       {globais['fidelity_overall']:.1f}%\n")
        f.write(f"  Necessidade:      {globais['necessity_overall']:.1f}%\n")
        f.write(f"  RedundÃ¢ncia:      {redundancia_pct:.1f}%\n")
        f.write(f"  Tamanho mÃ©dio:    {globais['mean_explanation_size']:.1f} features\n")
        f.write(f"  Cobertura:        {globais['coverage']:.1f}%\n\n")
        
        # InterpretaÃ§Ã£o curta e direta
        if globais['fidelity_overall'] >= 95 and globais['necessity_overall'] >= 90:
            avaliacao = "As explicaÃ§Ãµes sÃ£o fiÃ©is e minimais."
        elif globais['fidelity_overall'] >= 95:
            avaliacao = f"ExplicaÃ§Ãµes fiÃ©is, mas {redundancia_pct:.0f}% de redundÃ¢ncia (features desnecessÃ¡rias)."
        elif globais['necessity_overall'] >= 90:
            avaliacao = "ExplicaÃ§Ãµes minimais, porÃ©m fidelidade abaixo de 95%."
        else:
            avaliacao = "Qualidade insuficiente: ambas as mÃ©tricas requerem atenÃ§Ã£o."
        
        f.write(f"  AVALIAÃ‡ÃƒO: {avaliacao}\n\n")
        
        f.write("â”" * 80 + "\n\n")
        
        # SEÃ‡ÃƒO 6: LimitaÃ§Ãµes Observadas (NOVA)
        f.write("â”" * 80 + "\n")
        f.write("LIMITAÃ‡Ã•ES OBSERVADAS\n")
        f.write("â”" * 80 + "\n\n")
        
        limitacoes = []
        
        # LimitaÃ§Ã£o: RedundÃ¢ncia Alta
        if redundancia_pct > 20:
            limitacoes.append(f"  â€¢ Alta redundÃ¢ncia ({redundancia_pct:.0f}%): explicaÃ§Ãµes nÃ£o sÃ£o minimais.\n"
                            f"    PossÃ­vel causa: threshold de rejeiÃ§Ã£o muito conservador ou\n"
                            f"    features correlacionadas no dataset.")
        
        # LimitaÃ§Ã£o: Fidelidade Baixa
        if globais['fidelity_overall'] < 95:
            limitacoes.append(f"  â€¢ Fidelidade abaixo de 95% ({globais['fidelity_overall']:.1f}%): explicaÃ§Ãµes nÃ£o\n"
                            f"    reproduzem decisÃµes perfeitamente sob perturbaÃ§Ã£o.\n"
                            f"    PossÃ­vel causa: instabilidade do modelo ou features nÃ£o-explicativas\n"
                            f"    com alta influÃªncia em cenÃ¡rios perturbados.")
        
        # LimitaÃ§Ã£o: Variabilidade por tipo
        fid_pos = por_tipo['positive']['fidelity']
        fid_neg = por_tipo['negative']['fidelity']
        fid_rej = por_tipo['rejected']['fidelity']
        max_diff_fid = max(fid_pos, fid_neg, fid_rej) - min(fid_pos, fid_neg, fid_rej)
        if max_diff_fid > 10:
            limitacoes.append(f"  â€¢ Variabilidade entre tipos de decisÃ£o:\n"
                            f"    Positivas: {fid_pos:.1f}%, Negativas: {fid_neg:.1f}%, Rejeitadas: {fid_rej:.1f}%\n"
                            f"    DiferenÃ§a de {max_diff_fid:.1f}pp indica comportamento heterogÃªneo.")
        
        # LimitaÃ§Ã£o: Tamanho das explicaÃ§Ãµes
        if globais['mean_explanation_size'] > meta['num_features'] * 0.5:
            limitacoes.append(f"  â€¢ ExplicaÃ§Ãµes usam {globais['mean_explanation_size']:.1f} de {meta['num_features']} features ({globais['mean_explanation_size']/meta['num_features']*100:.0f}%):\n"
                            f"    CompactaÃ§Ã£o insuficiente para interpretabilidade prÃ¡tica.")
        
        # LimitaÃ§Ã£o: Cobertura incompleta
        if globais['coverage'] < 100:
            limitacoes.append(f"  â€¢ Cobertura incompleta ({globais['coverage']:.1f}%): {100-globais['coverage']:.1f}% das instÃ¢ncias\n"
                            f"    falharam. PossÃ­vel causa: timeouts ou erros numÃ©ricos.")
        
        # LimitaÃ§Ã£o: DistribuiÃ§Ã£o de tamanhos
        if globais['max_explanation_size'] >= meta['num_features']:
            limitacoes.append(f"  â€¢ ExplicaÃ§Ãµes completas detectadas (max={globais['max_explanation_size']} features):\n"
                            f"    MÃ©todo falhou em reduzir dimensionalidade em alguns casos.")
        
        if limitacoes:
            for lim in limitacoes:
                f.write(lim + "\n")
        else:
            f.write("  Nenhuma limitaÃ§Ã£o crÃ­tica detectada nesta validaÃ§Ã£o.\n\n")
        
        f.write("â”" * 80 + "\n\n")
        
        # SEÃ‡ÃƒO 7: RecomendaÃ§Ãµes PrÃ¡ticas
        f.write("â”" * 80 + "\n")
        f.write("RECOMENDAÃ‡Ã•ES\n")
        f.write("â”" * 80 + "\n\n")
        
        if globais['fidelity_overall'] >= 95 and globais['necessity_overall'] >= 85:
            f.write("  â€¢ MÃ©todo validado. ExplicaÃ§Ãµes apresentam qualidade aceitÃ¡vel.\n")
        else:
            f.write("  â€¢ Ajustar hiperparÃ¢metros (threshold de rejeiÃ§Ã£o, tolerÃ¢ncias).\n")
            f.write("  â€¢ Investigar instÃ¢ncias com baixa fidelidade ou alta redundÃ¢ncia.\n")
        
        if redundancia_pct > 20:
            f.write("  â€¢ Alta redundÃ¢ncia: considerar pÃ³s-processamento para remover features\n")
            f.write("    desnecessÃ¡rias (ex: backward selection).\n")
        
        if globais['coverage'] < 100:
            f.write("  â€¢ Investigar instÃ¢ncias que falharam na validaÃ§Ã£o.\n")
        
        f.write("\n")
        f.write("â”" * 80 + "\n")
        f.write(f"RelatÃ³rio gerado em: {meta['date']}\n")
        f.write("â”" * 80 + "\n")
    
    print(f"âœ“ RelatÃ³rio salvo: {report_path}")
    return report_path


def gerar_plots(resultado: Dict, metodo: str, dataset: str):
    """Gera os 6 plots de validaÃ§Ã£o."""
    
    # [ORGANIZAÃ‡ÃƒO] Estrutura: results/validation/{dataset}/{metodo}/
    output_dir = os.path.join(VALIDATION_RESULTS_DIR, metodo.lower(), dataset)
    os.makedirs(output_dir, exist_ok=True)
    
    globais = resultado['global_metrics']
    por_tipo = resultado['per_type_metrics']
    per_instance = resultado['per_instance_results']
    
    # Extrair dados
    sizes = [inst['explanation_size'] for inst in per_instance]
    fidelities = [inst['fidelity'] for inst in per_instance]
    tipos = []
    for inst in per_instance:
        if inst['rejected']:
            tipos.append('Rejeitada')
        elif inst['y_pred'] == 1:
            tipos.append('Positiva')
        else:
            tipos.append('Negativa')
    
    # Plot 1: Histograma de Fidelity
    plt.figure(figsize=(10, 6))
    plt.hist(fidelities, bins=20, edgecolor='black', alpha=0.7)
    plt.xlabel('Fidelity (%)', fontsize=12)
    plt.ylabel('FrequÃªncia', fontsize=12)
    plt.title(f'DistribuiÃ§Ã£o de Fidelity - {metodo.upper()} ({dataset})', fontsize=14, fontweight='bold')
    plt.axvline(globais['fidelity_overall'], color='red', linestyle='--', linewidth=2, label=f'MÃ©dia: {globais["fidelity_overall"]:.2f}%')
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_fidelity_histogram.png'), dpi=300)
    plt.close()
    
    # Plot 2: Boxplot de Tamanhos
    plt.figure(figsize=(8, 6))
    plt.boxplot(sizes, vert=True, patch_artist=True,
                boxprops=dict(facecolor='lightblue', alpha=0.7),
                medianprops=dict(color='red', linewidth=2))
    plt.ylabel('Tamanho da ExplicaÃ§Ã£o', fontsize=12)
    plt.title(f'DistribuiÃ§Ã£o de Tamanhos - {metodo.upper()} ({dataset})', fontsize=14, fontweight='bold')
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_boxplot_sizes.png'), dpi=300)
    plt.close()
    
    # Plot 3: Scatter Tamanho vs Fidelity
    plt.figure(figsize=(10, 6))
    cores = {'Positiva': 'green', 'Negativa': 'red', 'Rejeitada': 'orange'}
    for tipo in ['Positiva', 'Negativa', 'Rejeitada']:
        mask = [t == tipo for t in tipos]
        sizes_tipo = [s for s, m in zip(sizes, mask) if m]
        fid_tipo = [f for f, m in zip(fidelities, mask) if m]
        plt.scatter(sizes_tipo, fid_tipo, alpha=0.6, s=50, label=tipo, color=cores[tipo])
    
    plt.xlabel('Tamanho da ExplicaÃ§Ã£o', fontsize=12)
    plt.ylabel('Fidelity (%)', fontsize=12)
    plt.title(f'Tamanho vs Fidelity - {metodo.upper()} ({dataset})', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_size_vs_fidelity.png'), dpi=300)
    plt.close()
    
    # Plot 4: Heatmap Fidelity por Tipo
    fig, ax = plt.subplots(figsize=(8, 4))
    tipos_ordem = ['positive', 'negative', 'rejected']
    tipos_labels = ['Positiva', 'Negativa', 'Rejeitada']
    fidelities_por_tipo = [por_tipo[t]['fidelity'] for t in tipos_ordem]
    
    data_heatmap = np.array(fidelities_por_tipo).reshape(1, -1)
    sns.heatmap(data_heatmap, annot=True, fmt='.2f', cmap='RdYlGn', vmin=0, vmax=100,
                xticklabels=tipos_labels, yticklabels=[metodo], cbar_kws={'label': 'Fidelity (%)'})
    plt.title(f'Fidelity por Tipo de PrediÃ§Ã£o - {metodo.upper()} ({dataset})', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_heatmap_types.png'), dpi=300)
    plt.close()
    
    # Plot 5: Violin Plot de Tamanhos por Tipo
    plt.figure(figsize=(10, 6))
    df_plot = pd.DataFrame({'Tamanho': sizes, 'Tipo': tipos})
    ordem_tipos = ['Positiva', 'Negativa', 'Rejeitada']
    sns.violinplot(data=df_plot, x='Tipo', y='Tamanho', order=ordem_tipos, palette='Set2')
    plt.title(f'DistribuiÃ§Ã£o de Tamanhos por Tipo - {metodo.upper()} ({dataset})', fontsize=14, fontweight='bold')
    plt.ylabel('Tamanho da ExplicaÃ§Ã£o', fontsize=12)
    plt.xlabel('Tipo de PrediÃ§Ã£o', fontsize=12)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_violin_sizes.png'), dpi=300)
    plt.close()
    
    # Plot 6: MÃ©trica Reduction vs Fidelity
    plt.figure(figsize=(8, 6))
    reduction = globais['reduction_rate']
    fidelity = globais['fidelity_overall']
    
    plt.scatter([reduction], [fidelity], s=500, c='blue', alpha=0.6, edgecolors='black', linewidth=2)
    plt.annotate(metodo.upper(), (reduction, fidelity), fontsize=12, fontweight='bold',
                 xytext=(10, 10), textcoords='offset points')
    
    plt.xlabel('Taxa de ReduÃ§Ã£o (%)', fontsize=12)
    plt.ylabel('Fidelity (%)', fontsize=12)
    plt.title(f'EficiÃªncia: ReduÃ§Ã£o vs Fidelity - {metodo.upper()} ({dataset})', fontsize=14, fontweight='bold')
    plt.xlim(0, 100)
    plt.ylim(0, 105)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_reduction_vs_fidelity.png'), dpi=300)
    plt.close()
    
    print(f"âœ“ Plots salvos (6): {output_dir}/")


def menu_principal():
    """Menu interativo principal."""
    
    print("\n" + "=" * 70)
    print("           VALIDACAO DE EXPLICACOES - XAI COM REJEICAO")
    print("=" * 70)
    print("\n[1] Validar PEAB")
    print("[2] Validar PuLP (Ground Truth)")
    print("[3] Validar Anchor")
    print("[4] Validar MinExp")
    print("[5] Comparar Todos os Metodos (RECOMENDADO)")
    print("[0] Sair")
    
    opcao = input("\nOpcao: ").strip()
    
    if opcao == '0':
        print("Encerrando...")
        return
    
    # Selecionar dataset (reutilizar menu do PEAB)
    print("\n" + "-" * 70)
    print("Selecione o dataset para validacao...")
    print("-" * 70)
    
    try:
        from data.datasets import selecionar_dataset_e_classe
        nome_dataset, _, _, _, _ = selecionar_dataset_e_classe()
        
        if nome_dataset is None:
            print("âŒ Nenhum dataset selecionado.")
            return
    
    except Exception as e:
        print(f"âŒ Erro ao carregar menu de datasets: {e}")
        return
    
    # ConfiguraÃ§Ã£o automÃ¡tica (sem menu)
    print("\n" + "â”€" * 70)
    print("CONFIGURAÃ‡ÃƒO DA VALIDAÃ‡ÃƒO")
    print("â”€" * 70)
    print(f"â†’ EstratÃ©gia: {PERTURBATION_STRATEGY.upper()} (padrÃ£o fixo)")
    print(f"â†’ PerturbaÃ§Ãµes: Ajuste automÃ¡tico por tamanho do dataset")
    print(f"   â€¢ Datasets normais: {NUM_PERTURBATIONS_DEFAULT} perturbaÃ§Ãµes/instÃ¢ncia")
    print(f"   â€¢ Datasets grandes: {NUM_PERTURBATIONS_LARGE} perturbaÃ§Ãµes/instÃ¢ncia")
    print("â”€" * 70)
    
    # Executar validaÃ§Ã£o (sem passar n_perturbacoes e estrategia â†’ usa padrÃµes)
    if opcao in ['1', '2', '3', '4']:
        metodos_map = {'1': 'PEAB', '2': 'PuLP', '3': 'Anchor', '4': 'MinExp'}
        metodo = metodos_map[opcao]
        
        resultado = validar_metodo(metodo, nome_dataset)  # Usa padrÃµes automÃ¡ticos
        
        if resultado:
            salvar_json_validacao(resultado, metodo, nome_dataset)
            gerar_relatorio_txt(resultado, metodo, nome_dataset)
            gerar_plots(resultado, metodo, nome_dataset)
            
            print("\n" + "â•" * 70)
            print("âœ“ VALIDAÃ‡ÃƒO COMPLETA!")
            print("â•" * 70)
    
    elif opcao == '5':
        print("\nâ†’ Validando todos os mÃ©todos...")
        
        metodos = ['PEAB', 'PuLP', 'Anchor', 'MinExp']
        resultados = {}
        
        for metodo in metodos:
            resultado = validar_metodo(metodo, nome_dataset)  # Usa padrÃµes automÃ¡ticos
            
            if resultado:
                resultados[metodo] = resultado
                salvar_json_validacao(resultado, metodo, nome_dataset)
                gerar_relatorio_txt(resultado, metodo, nome_dataset)
                gerar_plots(resultado, metodo, nome_dataset)
        
        # Gerar comparaÃ§Ã£o
        if len(resultados) > 1:
            print("\nâ†’ Gerando comparaÃ§Ã£o entre mÃ©todos...")
            gerar_comparacao(resultados, nome_dataset)
        
        print("\n" + "â•" * 70)
        print("âœ“ VALIDAÃ‡ÃƒO COMPLETA PARA TODOS OS MÃ‰TODOS!")
        print("â•" * 70)
    
    else:
        print("âŒ OpÃ§Ã£o invÃ¡lida.")


def gerar_comparacao(resultados: Dict[str, Dict], dataset: str):
    """Gera relatÃ³rio e plots comparando todos os mÃ©todos."""
    
    output_dir = os.path.join(VALIDATION_RESULTS_DIR, dataset, "comparison")
    os.makedirs(output_dir, exist_ok=True)
    
    # Criar tabela comparativa
    report_path = os.path.join(output_dir, "comparison_report.txt")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("â•" * 80 + "\n")
        f.write("        COMPARAÃ‡ÃƒO DE MÃ‰TODOS - VALIDAÃ‡ÃƒO DE EXPLICAÃ‡Ã•ES\n")
        f.write("â•" * 80 + "\n\n")
        
        f.write(f"Dataset: {dataset}\n")
        f.write(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # Tabela comparativa
        f.write("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n")
        f.write("â”‚ MÃ©trica         â”‚   PEAB   â”‚   PuLP   â”‚  Anchor  â”‚  MinExp  â”‚\n")
        f.write("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n")
        
        metricas_chaves = [
            ('fidelity_overall', 'Fidelity (%)'),
            ('mean_explanation_size', 'Tamanho MÃ©dio'),
            ('reduction_rate', 'ReduÃ§Ã£o (%)'),
            ('validation_time_seconds', 'Tempo (s)')
        ]
        
        for chave, label in metricas_chaves:
            valores = []
            for metodo in ['PEAB', 'PuLP', 'Anchor', 'MinExp']:
                if metodo in resultados:
                    val = resultados[metodo]['global_metrics'][chave]
                    valores.append(f"{val:>8.2f}")
                else:
                    valores.append("    N/A ")
            
            f.write(f"â”‚ {label:<15} â”‚ {valores[0]} â”‚ {valores[1]} â”‚ {valores[2]} â”‚ {valores[3]} â”‚\n")
        
        f.write("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n")
        
        # Ranking
        f.write("RANKING POR FIDELITY:\n")
        f.write("â”€" * 80 + "\n")
        
        ranking = sorted(resultados.items(), 
                        key=lambda x: x[1]['global_metrics']['fidelity_overall'],
                        reverse=True)
        
        for i, (metodo, res) in enumerate(ranking, 1):
            fid = res['global_metrics']['fidelity_overall']
            size = res['global_metrics']['mean_explanation_size']
            f.write(f"{i}. {metodo:<10} - Fidelity: {fid:.2f}% | Tamanho: {size:.2f}\n")
        
        f.write("â•" * 80 + "\n")
    
    print(f"âœ“ ComparaÃ§Ã£o salva: {report_path}")
    
    # Plot comparativo
    plt.figure(figsize=(12, 6))
    
    metodos_nomes = list(resultados.keys())
    fidelities = [resultados[m]['global_metrics']['fidelity_overall'] for m in metodos_nomes]
    sizes = [resultados[m]['global_metrics']['mean_explanation_size'] for m in metodos_nomes]
    
    x = np.arange(len(metodos_nomes))
    width = 0.35
    
    fig, ax1 = plt.subplots(figsize=(10, 6))
    
    ax1.bar(x - width/2, fidelities, width, label='Fidelity (%)', color='skyblue', edgecolor='black')
    ax1.set_ylabel('Fidelity (%)', fontsize=12)
    ax1.set_ylim(0, 105)
    
    ax2 = ax1.twinx()
    ax2.bar(x + width/2, sizes, width, label='Tamanho MÃ©dio', color='lightcoral', edgecolor='black')
    ax2.set_ylabel('Tamanho MÃ©dio da ExplicaÃ§Ã£o', fontsize=12)
    
    ax1.set_xlabel('MÃ©todo', fontsize=12)
    ax1.set_title(f'ComparaÃ§Ã£o de MÃ©todos - {dataset}', fontsize=14, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(metodos_nomes)
    ax1.legend(loc='upper left')
    ax2.legend(loc='upper right')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'plot_methods_comparison.png'), dpi=300)
    plt.close()
    
    print(f"âœ“ Plot comparativo salvo: {output_dir}/")


if __name__ == '__main__':
    menu_principal()
